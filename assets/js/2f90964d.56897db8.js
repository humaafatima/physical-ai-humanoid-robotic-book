"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[589],{6279:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"09-vla-models","title":"Chapter 9 - Vision-Language-Action Models","description":"Introduction to Vision-Language-Action (VLA) models that bridge natural language, visual perception, and robotic control.","source":"@site/docs/09-vla-models.md","sourceDirName":".","slug":"/09-vla-models","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/humaafatima/physical-ai-humanoid-robotics-book/tree/main/book/docs/09-vla-models.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"id":"09-vla-models","title":"Chapter 9 - Vision-Language-Action Models","sidebar_label":"9. VLA Models","word_count_target":2000,"word_count_actual":2340,"status":"drafted","learning_objectives":["Understand Vision-Language-Action (VLA) model architectures","Deploy multimodal foundation models for robotics","Ground language commands to robotic actions","Implement open-vocabulary object manipulation"]},"sidebar":"tutorialSidebar","previous":{"title":"8. Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"},"next":{"title":"10. Voice to Action","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"}}');var o=t(4848),a=t(8453);const s={id:"09-vla-models",title:"Chapter 9 - Vision-Language-Action Models",sidebar_label:"9. VLA Models",word_count_target:2e3,word_count_actual:2340,status:"drafted",learning_objectives:["Understand Vision-Language-Action (VLA) model architectures","Deploy multimodal foundation models for robotics","Ground language commands to robotic actions","Implement open-vocabulary object manipulation"]},r="Chapter 9: Vision-Language-Action Models",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"9.1 The VLA Paradigm",id:"91-the-vla-paradigm",level:2},{value:"9.2 Deploying VLA Models",id:"92-deploying-vla-models",level:2},{value:"9.3 Language Grounding and Action Prediction",id:"93-language-grounding-and-action-prediction",level:2},{value:"9.4 Open-Vocabulary Object Manipulation",id:"94-open-vocabulary-object-manipulation",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-9-vision-language-action-models",children:"Chapter 9: Vision-Language-Action Models"})}),"\n",(0,o.jsxs)(n.admonition,{title:"Chapter Overview",type:"info",children:[(0,o.jsx)(n.p,{children:"Introduction to Vision-Language-Action (VLA) models that bridge natural language, visual perception, and robotic control."}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Word Target"}),": 1,900-2,100 words\n",(0,o.jsx)(n.strong,{children:"Code Examples"}),": 4 (VLA inference, language grounding, object grounding, action prediction)"]})]}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Explain VLA model architectures and training paradigms"}),"\n",(0,o.jsx)(n.li,{children:"Deploy pre-trained VLA models (RT-2, PaLM-E, OpenVLA)"}),"\n",(0,o.jsx)(n.li,{children:"Ground natural language commands to robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Use vision-language models for open-vocabulary object detection"}),"\n",(0,o.jsx)(n.li,{children:"Fine-tune VLA models on custom robotic tasks"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"91-the-vla-paradigm",children:"9.1 The VLA Paradigm"}),"\n",(0,o.jsx)(n.p,{children:'Traditional robotic control requires task-specific policies\u2014neural networks trained from scratch on demonstrations for each individual task like "pick up the red cup" or "open the drawer." This approach demands thousands of demonstrations per task, fails to generalize across objects or environments, and requires complete retraining when tasks change slightly. Vision-Language-Action (VLA) models represent a paradigm shift: large-scale foundation models that unify visual perception, natural language understanding, and robotic control into a single architecture capable of zero-shot generalization to novel tasks, objects, and instructions.'}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"From task-specific policies to foundation models"}),' mirrors the revolution that occurred in natural language processing with GPT and computer vision with CLIP. Instead of training separate policies for each task, VLA models are pre-trained on massive diverse datasets\u2014millions of robotic demonstrations across tasks, combined with billions of web images and text describing human activities and object interactions. This web-scale pre-training teaches the model broad knowledge about objects, actions, spatial relationships, and task semantics. At deployment, the same model can handle "pick up the cup," "move the laptop to the left," or "open the top drawer" without task-specific fine-tuning, leveraging its general understanding of objects, verbs, and spatial concepts. The key insight: language provides a universal interface for task specification, while vision-language pre-training on internet data transfers knowledge about the physical world to robotic manipulation.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language-Action architecture"})," typically follows a transformer-based design that processes multimodal inputs and outputs actions. The model receives three inputs: ",(0,o.jsx)(n.strong,{children:"visual observations"})," (RGB images from cameras, often multiple viewpoints), ",(0,o.jsx)(n.strong,{children:"language instructions"})," (natural language task descriptions tokenized as text), and ",(0,o.jsx)(n.strong,{children:"robot state"})," (proprioceptive information like joint positions and gripper state). These inputs get encoded through modality-specific encoders\u2014vision transformers (ViT) for images, text transformers for language\u2014then fused in a shared transformer backbone. The model outputs ",(0,o.jsx)(n.strong,{children:"actions"}),'\u2014continuous or discrete control commands like end-effector velocities, joint angles, or gripper open/close signals. During training, the model learns to predict actions that maximize task success given visual-language context, essentially learning a conditional distribution P(action | image, language, robot_state). The transformer architecture enables attention mechanisms that ground language tokens to visual regions (when the instruction says "red cup," attention focuses on red cup pixels) and reason about temporal sequences (multi-step tasks).']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Training data requirements"})," scale beyond traditional robotics. VLA models require two data types: ",(0,o.jsx)(n.strong,{children:"robotic demonstration data"})," (trajectories of robot actions paired with camera observations and task descriptions) and ",(0,o.jsx)(n.strong,{children:"vision-language data"})," (web images with captions, instructional videos, object-centric datasets). Robotic data teaches motor control and task execution\u2014datasets like Open X-Embodiment aggregate millions of demonstrations across different robots, grippers, and environments. Vision-language data teaches semantic understanding\u2014recognizing objects, understanding verbs and spatial relations, reasoning about physical properties. Models like RT-2 co-train on both: the vision-language pre-training provides world knowledge and generalization, while robotic fine-tuning specializes the model for embodied control. This two-stage approach means VLA models can recognize and manipulate objects they've never physically interacted with, as long as those objects appear in web data. The data requirements are substantial\u2014millions of robotic episodes, billions of image-text pairs\u2014but enable unprecedented generalization."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Emergent capabilities"})," arise from scale and multimodal pre-training. ",(0,o.jsx)(n.strong,{children:"Zero-shot generalization"}),' lets VLA models handle novel objects (manipulating a spatula after training only on spoons), novel instructions (understanding synonyms and paraphrases like "grasp" vs "pick up"), and novel compositions (combining known skills in new ways like "stack the blue block on the red block"). ',(0,o.jsx)(n.strong,{children:"Chain-of-thought reasoning"}),' enables models to decompose complex instructions into subtasks\u2014given "prepare the table," the model might infer it needs to fetch plates, arrange utensils, and center items. ',(0,o.jsx)(n.strong,{children:"Affordance understanding"})," emerges without explicit training: models learn which object parts are graspable, which surfaces support placing objects, and which actions are physically feasible. These capabilities emerge from the combination of scale (model size, data size) and the rich semantic grounding provided by language-vision pre-training."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Current VLA models"})," represent the state of the art. ",(0,o.jsx)(n.strong,{children:"RT-1 (Robotics Transformer 1)"})," demonstrated that transformers could learn robotic control end-to-end, training on 130k demonstrations to achieve 97% success on everyday tasks. ",(0,o.jsx)(n.strong,{children:"RT-2"})," extended this by co-training on web data, enabling zero-shot generalization\u2014it successfully manipulated objects like toy dinosaurs not present in robotic training data. ",(0,o.jsx)(n.strong,{children:"PaLM-E"})," integrated Google's PaLM language model with robotic embodiment, creating a 562B parameter model that reasons about tasks using both robotic sensor data and language. ",(0,o.jsx)(n.strong,{children:"OpenVLA"})," provides an open-source 7B parameter model trained on the Open X-Embodiment dataset, offering strong performance with accessible compute requirements. ",(0,o.jsx)(n.strong,{children:"Octo"})," focuses on generalist policies across robot morphologies, learning action representations that transfer between different robot arms and grippers. These models establish that foundation models for robotics are viable, practical, and rapidly improving."]}),"\n",(0,o.jsx)(n.h2,{id:"92-deploying-vla-models",children:"9.2 Deploying VLA Models"}),"\n",(0,o.jsx)(n.p,{children:"Deploying VLA models on humanoid robots involves practical tradeoffs between model capability, computational resources, and real-time requirements. While cloud deployment offers unlimited compute for large models, edge deployment on robot hardware provides low latency and offline operation\u2014critical for responsive manipulation and navigation in environments without reliable connectivity."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Model selection"})," depends on deployment constraints and task requirements. ",(0,o.jsx)(n.strong,{children:"Cloud deployment"}),' suits scenarios where latency isn\'t critical (planning-level tasks, infrequent decisions) or when model size exceeds edge compute capacity. Large models like PaLM-E (562B parameters) run exclusively in cloud infrastructure, requiring API calls that introduce 500ms-2s round-trip latency. This works for high-level task planning ("how should I clean this room?") but fails for reactive control loops (grasping, balance correction). ',(0,o.jsx)(n.strong,{children:"Edge deployment"})," on robot hardware\u2014NVIDIA Jetson Orin (up to 275 TOPS), embedded GPUs, or custom accelerators\u2014enables low-latency inference (50-200ms) suitable for closed-loop control. Models like OpenVLA (7B parameters) and Octo (93M parameters) target edge deployment, fitting in 16-32GB GPU memory with optimizations. The selection criteria: if the task requires real-time feedback (manipulation, walking balance), deploy on edge; if it's high-level reasoning tolerating latency, consider cloud. Hybrid approaches work well\u2014cloud models for planning, edge models for execution."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Quantization and optimization for Jetson"})," reduce model size and increase inference speed without catastrophic accuracy loss. ",(0,o.jsx)(n.strong,{children:"Quantization"})," converts model weights from FP32 (32-bit floating point) to FP16 (16-bit) or INT8 (8-bit integers), reducing memory footprint by 2-4\xd7 and accelerating inference through specialized hardware instructions (Tensor Cores, INT8 ops). Post-training quantization (PTQ) converts trained models without retraining\u2014tools like TensorRT (NVIDIA's inference optimizer) handle this automatically, analyzing activation distributions to minimize quantization error. Quantization-aware training (QAT) inserts fake quantization operations during training, teaching the model to maintain accuracy under quantization. For VLA models, FP16 quantization typically loses <1% accuracy and enables real-time inference on Jetson Orin; INT8 quantization achieves 3-4\xd7 speedup but may degrade performance by 2-5%, requiring validation. ",(0,o.jsx)(n.strong,{children:"TensorRT optimization"})," goes beyond quantization: it fuses layers (combining convolution+batch_norm+activation into single ops), optimizes memory layout, and generates platform-specific CUDA kernels. The workflow: export VLA model to ONNX format, convert ONNX to TensorRT engine with quantization flags, deploy the optimized engine with TensorRT runtime. This pipeline can reduce inference latency from 500ms to 100ms for a 7B parameter model on Jetson Orin."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 integration patterns"})," connect VLA inference to robot perception and control. The typical architecture: a ",(0,o.jsx)(n.strong,{children:"VLA inference node"})," subscribes to camera image topics (",(0,o.jsx)(n.code,{children:"sensor_msgs/Image"}),") and language command topics (",(0,o.jsx)(n.code,{children:"std_msgs/String"}),"), runs inference, and publishes action commands (",(0,o.jsx)(n.code,{children:"geometry_msgs/Twist"})," for velocity, ",(0,o.jsx)(n.code,{children:"trajectory_msgs/JointTrajectory"})," for arm motion, ",(0,o.jsx)(n.code,{children:"std_msgs/Bool"}),' for gripper). The node manages GPU memory (pre-allocating buffers), runs inference asynchronously (to avoid blocking ROS callbacks), and handles coordinate frame transforms (converting model outputs from camera frame to robot base frame using tf2). For efficiency, the node can buffer images and run inference at a lower rate than camera publication (e.g., process every 5th frame at 6 Hz rather than every frame at 30 Hz). Action outputs often require post-processing\u2014VLA models may output end-effector poses that need inverse kinematics to compute joint angles, or they may output relative actions ("move 10cm forward") that require integration with odometry. ROS 2 action servers provide a clean interface: the VLA node can be a client to MoveIt 2 (for motion planning) or Nav2 (for navigation), translating high-level VLA decisions into robust execution.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Latency considerations"})," determine whether VLA models can operate in closed-loop control. Manipulation control loops run at 10-100 Hz (10-100ms cycles), requiring VLA inference latency under 50ms to leave time for sensing, control computation, and actuation. Current edge-deployed VLA models achieve 100-200ms inference latency on Jetson Orin, suitable for semi-reactive tasks (grasping stationary objects, waypoint navigation) but too slow for dynamic tasks (catching thrown objects, balancing during disturbances). Strategies to mitigate latency: ",(0,o.jsx)(n.strong,{children:"temporal batching"})," (run inference less frequently, use last action for intermediate steps), ",(0,o.jsx)(n.strong,{children:"action chunking"})," (predict sequences of future actions in one inference, execute them open-loop), ",(0,o.jsx)(n.strong,{children:"hierarchical control"})," (VLA generates high-level goals, fast low-level controller executes), and ",(0,o.jsx)(n.strong,{children:"speculative execution"})," (pre-compute likely next actions). For humanoids, a practical pattern: VLA runs at 5-10 Hz for task decisions, while 100+ Hz low-level controllers handle balance and immediate obstacle avoidance."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Prompt engineering for robotic tasks"})," significantly impacts VLA success rates. Unlike general chatbots, robotic VLA models benefit from structured prompts that specify task details and environmental context. Effective prompts include: ",(0,o.jsx)(n.strong,{children:"object specificity"}),' ("pick up the red mug with the handle" vs vague "pick up the mug"), ',(0,o.jsx)(n.strong,{children:"spatial clarity"}),' ("move 20cm to the left" vs "move left"), ',(0,o.jsx)(n.strong,{children:"sequential structure"}),' for multi-step tasks ("first open the drawer, then place the cup inside"), and ',(0,o.jsx)(n.strong,{children:"environmental context"}),' ("there are two cups; grasp the one closer to you"). Some VLA models support ',(0,o.jsx)(n.strong,{children:"few-shot prompting"}),'\u2014providing example instruction-action pairs before the target task to bias the model toward desired behavior. Prompt templates help: "Task: [verb] the [object] [spatial_relation] | Object location: [coordinates] | Robot state: [current_pose]". Experimenting with prompt variations and logging success rates guides optimization\u2014small wording changes can have outsized effects on model behavior.']}),"\n",(0,o.jsx)(n.h2,{id:"93-language-grounding-and-action-prediction",children:"9.3 Language Grounding and Action Prediction"}),"\n",(0,o.jsx)(n.p,{children:"The defining capability of VLA models is transforming natural language instructions\u2014inherently abstract and symbolic\u2014into concrete robot actions grounded in the physical world. This requires understanding not just word meanings but spatial relationships, object properties, action semantics, and the robot's embodiment constraints. Language grounding bridges the gap between human intent expressed in words and executable motor commands."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Natural language command parsing"}),' for robotics differs from general NLP because instructions must be executable\u2014vague language needs disambiguation, implicit assumptions need surfacing, and impossible requests need rejection. A command like "bring me the cup" contains several grounding challenges: which cup (if multiple exist), where is "me" (requires person detection and localization), what trajectory (collision-free path planning), and what constitutes success (placed within reach? handed directly?). VLA models trained on diverse robotic data learn to parse these commands by pattern matching against training examples, but parsing errors\u2014misidentifying objects, misunderstanding verbs, ignoring spatial constraints\u2014remain common failure modes. Robust systems combine VLA parsing with validation: after parsing "move the laptop left," verify a laptop is visible, confirm "left" is relative to robot\'s viewpoint, and check that the target location is reachable and collision-free before execution.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grounding language to action sequences"}),' involves mapping linguistic concepts to motor primitives. Commands like "pick up" ground to a sequence: approach object, open gripper, move to pre-grasp pose, close gripper, lift. "Move left" grounds to a base motion command with negative y-axis velocity. VLA models learn these groundings implicitly through training on demonstration data\u2014when the model observes humans saying "pick up" while demonstrators execute grasp sequences, it associates the phrase with that action pattern. The challenge: compositional generalization. Can a model trained on "pick up the cup" and "move the box left" successfully execute "pick up the cup and move it left"? Modern VLA models exhibit some compositional understanding, but complex multi-step tasks with novel combinations often fail. This motivates hierarchical approaches: use VLA to decompose high-level commands into subtasks ("pick up," "move," "place"), then use specialized low-level controllers for each subtask, ensuring reliable execution of known primitives while leveraging VLA\'s language understanding for task decomposition.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Spatial reasoning"}),' enables understanding relational descriptions essential for manipulation and navigation. When a human says "place the cup on the table," the model must: detect the cup (object grounding), detect the table (surface grounding), understand "on" means vertically above with contact (spatial relation), and compute a placement pose satisfying this relation. Spatial language includes ',(0,o.jsx)(n.strong,{children:"locative relations"}),' ("on," "in," "under," "near"), ',(0,o.jsx)(n.strong,{children:"directional terms"}),' ("left," "right," "forward," "behind"), ',(0,o.jsx)(n.strong,{children:"distance specifications"}),' ("close," "far," "next to"), and ',(0,o.jsx)(n.strong,{children:"reference frames"}),' (allocentric "north of the table" vs egocentric "to my left"). VLA models learn these through multimodal grounding\u2014visual attention mechanisms identify relevant objects while language tokens specify their relationships. Attention maps reveal grounding: when processing "left of the red cup," attention highlights red objects and regions to their left. However, spatial reasoning failures are common: models may confuse reference frames (robot\'s left vs human\'s left), misjudge distances, or fail with complex nested relations ("on the box next to the table"). Providing explicit spatial coordinates in prompts (when available) improves reliability: "move to position (x=1.2, y=0.3)" vs "move forward a bit."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Handling ambiguity and clarification requests"}),' distinguishes robust VLA systems from brittle ones. Ambiguous commands arise constantly: "get the cup" when three cups are visible, "move left" without specifying how far, "open the door" when the robot lacks door-opening capabilities. Production VLA systems should detect ambiguity\u2014through confidence thresholds (low-confidence predictions trigger clarification), explicit ambiguity checks (object detector finds multiple matches), or feasibility validation (requested action violates constraints)\u2014and request clarification from users. Clarification mechanisms include: ',(0,o.jsx)(n.strong,{children:"object disambiguation"}),' (highlight candidates on screen or describe them: "I see two cups\u2014red one on the table, blue one on the counter. Which?"), ',(0,o.jsx)(n.strong,{children:"parameter elicitation"}),' (ask "how far left?"), ',(0,o.jsx)(n.strong,{children:"capability negotiation"}),' ("I cannot open doors; should I approach and wait?"). Implementing clarification requires multimodal interaction: VLA generates clarification prompts, speech synthesis vocalizes them, speech recognition captures responses, and VLA processes responses to refine the command. This interactive loop transforms VLA from one-shot command execution to collaborative task completion.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Mapping VLA outputs to ROS 2 actions"})," requires translating model outputs\u2014typically floating-point vectors representing poses, velocities, or gripper states\u2014into ROS messages and action goals. VLA models output actions in various formats: ",(0,o.jsx)(n.strong,{children:"end-effector deltas"})," (relative movements: \u0394x, \u0394y, \u0394z, \u0394roll, \u0394pitch, \u0394yaw, gripper_open), ",(0,o.jsx)(n.strong,{children:"absolute poses"})," (target end-effector pose in camera or base frame), ",(0,o.jsx)(n.strong,{children:"joint velocities"})," (direct motor commands), or ",(0,o.jsx)(n.strong,{children:"high-level goals"}),' (semantic actions like "grasp" that trigger downstream planners). The ROS integration layer converts these: end-effector poses use inverse kinematics (via MoveIt 2 or analytic solvers) to compute joint angles, deltas integrate with current odometry to compute absolute targets, and high-level goals trigger ROS action clients (e.g., a "navigate to kitchen" goal invokes Nav2\'s NavigateToPose action). Coordinate frame transforms via tf2 are essential\u2014VLA models typically output in camera frame, but motion planners need base frame or world frame. The integration node subscribes to VLA action topics, applies transforms, validates feasibility (collision checking, reachability), and publishes or calls the appropriate ROS 2 interfaces for execution.']}),"\n",(0,o.jsx)(n.h2,{id:"94-open-vocabulary-object-manipulation",children:"9.4 Open-Vocabulary Object Manipulation"}),"\n",(0,o.jsx)(n.p,{children:"Traditional robotic manipulation requires training object-specific detectors and grasp planners for each object the robot might encounter\u2014a brittle approach that fails when novel objects appear. Open-vocabulary manipulation, enabled by VLA models, allows robots to detect, reason about, and manipulate arbitrary objects described in natural language, even if those objects never appeared in the robot's training data. This capability transforms humanoid robots from special-purpose machines into general-purpose assistants."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zero-shot object detection with vision-language models"}),' eliminates the need for object-specific training. Instead of training a detector to recognize "cup" by showing it thousands of cup images, vision-language models like CLIP, OWL-ViT, or Grounding DINO learn joint embeddings of images and text from web-scale data, enabling them to detect any object described in text. The workflow: provide a text query ("red coffee mug"), the model computes text embeddings, slides a detection window across the image computing image region embeddings, and returns regions where image-text similarity exceeds a threshold\u2014those regions contain the queried object. This approach generalizes to objects outside training data: if the model learned "mug" from web images and "spatula" from others, it can detect "red spatula" through compositional reasoning about color and object type. For robotics, this enables commands like "pick up the yellow screwdriver" to succeed even if the robot never specifically trained on yellow screwdrivers, as long as it learned "yellow" and "screwdriver" independently. The detection quality depends on distinctiveness\u2014unique objects (unusual tools, specific brands) detect reliably, while generic objects (plain white cups among similar ones) require additional context or demonstrations.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Object grounding in 3D space"}),' connects 2D visual detections to 3D poses required for manipulation. A detected bounding box tells you where pixels are, not where the physical object is in 3D space relative to the robot. The grounding pipeline: run open-vocabulary detection on RGB images to get 2D bounding boxes, align depth data from RGB-D cameras to find depth values within each box, project detected pixels to 3D point clouds using camera intrinsics, segment the point cloud to isolate the target object, fit geometric primitives or compute oriented bounding boxes to estimate 6D pose (position + orientation), and transform to the robot base frame via tf2. This gives the robot actionable information: "the red mug is at position (0.4, -0.2, 0.8) meters from my base, oriented upright." For robust grounding, systems often fuse detections across multiple camera viewpoints\u2014errors in one view get corrected by others, and occlusions in one view appear visible in another. Semantic segmentation (Chapter 7) enhances grounding by providing pixel-precise object boundaries rather than rectangular boxes, improving pose estimation accuracy especially for irregularly shaped objects.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Manipulation primitives"})," provide the building blocks VLA models compose into task solutions. Core primitives include: ",(0,o.jsx)(n.strong,{children:"pick"})," (approach, grasp, lift), ",(0,o.jsx)(n.strong,{children:"place"})," (approach target location, lower, release), ",(0,o.jsx)(n.strong,{children:"push"})," (contact object, apply force in direction), ",(0,o.jsx)(n.strong,{children:"pull"})," (grasp handle/edge, retract), ",(0,o.jsx)(n.strong,{children:"pour"})," (tilt grasped container over target), ",(0,o.jsx)(n.strong,{children:"open/close"})," (articulated object manipulation like drawers, doors), and ",(0,o.jsx)(n.strong,{children:"handover"}),' (present grasped object to human). Each primitive encapsulates motion planning, force control, and failure handling\u2014pick includes grasp pose computation (where to grip), pre-grasp approach (collision-free trajectory), grasp execution (close gripper with force limits), and lift verification (detect grasp success by monitoring gripper force/position). VLA models learn to sequence primitives: "clear the table" might decompose to repeated pick-and-place operations for each detected object. The open-vocabulary aspect: VLA models ground language commands to these primitives ("move the stapler to the drawer" \u2192 navigate-to-stapler, pick(stapler), navigate-to-drawer, open(drawer), place(stapler, in-drawer), close(drawer)) without requiring task-specific programming.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Combining VLA with motion planning (MoveIt 2)"}),' creates a powerful hybrid: VLA handles high-level task understanding and object grounding, while MoveIt 2 ensures safe, collision-free execution. The integration pattern: VLA outputs target end-effector poses or object manipulation goals, the ROS integration layer converts these to MoveIt 2 motion planning requests (PlanningSceneInterface to represent obstacles, PlanningInterface to compute trajectories, ExecuteTrajectory action to execute), MoveIt 2 computes collision-free joint-space trajectories respecting kinematic constraints, and the robot controller executes the trajectory while monitoring for collisions or joint limits. This division of labor leverages the strengths of each component\u2014VLA\'s semantic understanding and generalization versus MoveIt 2\'s geometric reasoning and safety guarantees. For instance, VLA might decide "grasp the cup from above," compute a grasp pose, and pass it to MoveIt 2; MoveIt 2 verifies reachability, plans a collision-free approach avoiding the nearby laptop, and executes the motion. If MoveIt 2 reports "unreachable" or "collision," the VLA can replan\u2014try a different grasp angle, ask the human to move obstacles, or abort gracefully.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Failure detection and recovery"})," handle the inevitable errors in real-world manipulation\u2014missed grasps, dropped objects, occluded targets, or environmental changes. VLA systems should monitor execution and detect failures through sensor feedback: grasp failure (gripper closes without resistance, indicating missed grasp), drop detection (sudden force drop during transport), localization failure (visual tracking loses object), or collision events (unexpected contact forces). Recovery strategies mirror those in navigation (Chapter 8): retry with adjusted parameters (re-grasp from different angle), use alternate primitives (push object to better position before grasping), request human assistance (ask user to hand object directly), or abort and report failure. Advanced VLA models exhibit learned recovery behaviors\u2014if initial grasp fails, try grasping a different part of the object; if object slips, adjust grip before proceeding. The key is graceful degradation: rather than silently failing or causing damage, the system detects problems, attempts reasonable recoveries, and communicates clearly when human intervention is needed."]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"VLA Inference"}),": Deploy OpenVLA or RT-2 and test with basic manipulation commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language Grounding"}),': Map commands like "pick up the red cup" to action sequences']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Open-Vocabulary Detection"}),": Detect arbitrary objects using VLM zero-shot capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prompt Engineering"}),": Design prompts that improve VLA task success rates"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fine-Tuning"}),": Collect demonstrations and fine-tune VLA on a custom task"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"VLA models enable open-ended natural language control of robots"}),"\n",(0,o.jsx)(n.li,{children:"Pre-trained foundation models exhibit zero-shot generalization"}),"\n",(0,o.jsx)(n.li,{children:"Grounding language to actions requires spatial and semantic reasoning"}),"\n",(0,o.jsx)(n.li,{children:"Combining VLA with classical planners improves reliability"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'RT-2 paper: "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"'}),"\n",(0,o.jsx)(n.li,{children:'PaLM-E paper: "PaLM-E: An Embodied Multimodal Language Model"'}),"\n",(0,o.jsx)(n.li,{children:"OpenVLA project and model zoo"}),"\n",(0,o.jsx)(n.li,{children:"Foundation models for robotics survey papers"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Status"}),": Draft complete (2,340 words)"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);