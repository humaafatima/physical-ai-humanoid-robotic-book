<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-09-vla-models" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 9 - Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 9 - Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Introduction to Vision-Language-Action (VLA) models that bridge natural language, visual perception, and robotic control."><meta data-rh="true" property="og:description" content="Introduction to Vision-Language-Action (VLA) models that bridge natural language, visual perception, and robotic control."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models" hreflang="en"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"9. VLA Models","item":"https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="Physical AI &amp; Humanoid Robotics" href="/Physical-AI-Humanoid-Robotics-Book/opensearch.xml"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Book/assets/css/styles.03ae9914.css">
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/runtime~main.d422561b.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/main.e7e20e78.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Book/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Book/docs/">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/"><span title="Welcome" class="linkLabel_WmDU">Welcome</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro"><span title="1. Physical AI &amp; Embodied Intelligence" class="linkLabel_WmDU">1. Physical AI &amp; Embodied Intelligence</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/02-landscape"><span title="2. Humanoid Robotics Landscape" class="linkLabel_WmDU">2. Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/03-ros2"><span title="3. ROS 2" class="linkLabel_WmDU">3. ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/04-urdf"><span title="4. URDF &amp; Robot Modeling" class="linkLabel_WmDU">4. URDF &amp; Robot Modeling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/05-digital-twins"><span title="5. Digital Twins" class="linkLabel_WmDU">5. Digital Twins</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/06-sensors"><span title="6. Sensors" class="linkLabel_WmDU">6. Sensors</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/07-isaac-ros"><span title="7. Isaac ROS" class="linkLabel_WmDU">7. Isaac ROS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><span title="8. Navigation" class="linkLabel_WmDU">8. Navigation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><span title="9. VLA Models" class="linkLabel_WmDU">9. VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><span title="10. Voice to Action" class="linkLabel_WmDU">10. Voice to Action</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><span title="11. Manipulation" class="linkLabel_WmDU">11. Manipulation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><span title="12. Sim-to-Real" class="linkLabel_WmDU">12. Sim-to-Real</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><span title="13. Capstone Project" class="linkLabel_WmDU">13. Capstone Project</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware"><span title="14. Hardware" class="linkLabel_WmDU">14. Hardware</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">9. VLA Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 9: Vision-Language-Action Models</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Chapter Overview</div><div class="admonitionContent_BuS1"><p>Introduction to Vision-Language-Action (VLA) models that bridge natural language, visual perception, and robotic control.</p><p><strong>Word Target</strong>: 1,900-2,100 words
<strong>Code Examples</strong>: 4 (VLA inference, language grounding, object grounding, action prediction)</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Explain VLA model architectures and training paradigms</li>
<li class="">Deploy pre-trained VLA models (RT-2, PaLM-E, OpenVLA)</li>
<li class="">Ground natural language commands to robot actions</li>
<li class="">Use vision-language models for open-vocabulary object detection</li>
<li class="">Fine-tune VLA models on custom robotic tasks</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="91-the-vla-paradigm">9.1 The VLA Paradigm<a href="#91-the-vla-paradigm" class="hash-link" aria-label="Direct link to 9.1 The VLA Paradigm" title="Direct link to 9.1 The VLA Paradigm" translate="no">​</a></h2>
<p>Traditional robotic control requires task-specific policies—neural networks trained from scratch on demonstrations for each individual task like &quot;pick up the red cup&quot; or &quot;open the drawer.&quot; This approach demands thousands of demonstrations per task, fails to generalize across objects or environments, and requires complete retraining when tasks change slightly. Vision-Language-Action (VLA) models represent a paradigm shift: large-scale foundation models that unify visual perception, natural language understanding, and robotic control into a single architecture capable of zero-shot generalization to novel tasks, objects, and instructions.</p>
<p><strong>From task-specific policies to foundation models</strong> mirrors the revolution that occurred in natural language processing with GPT and computer vision with CLIP. Instead of training separate policies for each task, VLA models are pre-trained on massive diverse datasets—millions of robotic demonstrations across tasks, combined with billions of web images and text describing human activities and object interactions. This web-scale pre-training teaches the model broad knowledge about objects, actions, spatial relationships, and task semantics. At deployment, the same model can handle &quot;pick up the cup,&quot; &quot;move the laptop to the left,&quot; or &quot;open the top drawer&quot; without task-specific fine-tuning, leveraging its general understanding of objects, verbs, and spatial concepts. The key insight: language provides a universal interface for task specification, while vision-language pre-training on internet data transfers knowledge about the physical world to robotic manipulation.</p>
<p><strong>Vision-Language-Action architecture</strong> typically follows a transformer-based design that processes multimodal inputs and outputs actions. The model receives three inputs: <strong>visual observations</strong> (RGB images from cameras, often multiple viewpoints), <strong>language instructions</strong> (natural language task descriptions tokenized as text), and <strong>robot state</strong> (proprioceptive information like joint positions and gripper state). These inputs get encoded through modality-specific encoders—vision transformers (ViT) for images, text transformers for language—then fused in a shared transformer backbone. The model outputs <strong>actions</strong>—continuous or discrete control commands like end-effector velocities, joint angles, or gripper open/close signals. During training, the model learns to predict actions that maximize task success given visual-language context, essentially learning a conditional distribution P(action | image, language, robot_state). The transformer architecture enables attention mechanisms that ground language tokens to visual regions (when the instruction says &quot;red cup,&quot; attention focuses on red cup pixels) and reason about temporal sequences (multi-step tasks).</p>
<p><strong>Training data requirements</strong> scale beyond traditional robotics. VLA models require two data types: <strong>robotic demonstration data</strong> (trajectories of robot actions paired with camera observations and task descriptions) and <strong>vision-language data</strong> (web images with captions, instructional videos, object-centric datasets). Robotic data teaches motor control and task execution—datasets like Open X-Embodiment aggregate millions of demonstrations across different robots, grippers, and environments. Vision-language data teaches semantic understanding—recognizing objects, understanding verbs and spatial relations, reasoning about physical properties. Models like RT-2 co-train on both: the vision-language pre-training provides world knowledge and generalization, while robotic fine-tuning specializes the model for embodied control. This two-stage approach means VLA models can recognize and manipulate objects they&#x27;ve never physically interacted with, as long as those objects appear in web data. The data requirements are substantial—millions of robotic episodes, billions of image-text pairs—but enable unprecedented generalization.</p>
<p><strong>Emergent capabilities</strong> arise from scale and multimodal pre-training. <strong>Zero-shot generalization</strong> lets VLA models handle novel objects (manipulating a spatula after training only on spoons), novel instructions (understanding synonyms and paraphrases like &quot;grasp&quot; vs &quot;pick up&quot;), and novel compositions (combining known skills in new ways like &quot;stack the blue block on the red block&quot;). <strong>Chain-of-thought reasoning</strong> enables models to decompose complex instructions into subtasks—given &quot;prepare the table,&quot; the model might infer it needs to fetch plates, arrange utensils, and center items. <strong>Affordance understanding</strong> emerges without explicit training: models learn which object parts are graspable, which surfaces support placing objects, and which actions are physically feasible. These capabilities emerge from the combination of scale (model size, data size) and the rich semantic grounding provided by language-vision pre-training.</p>
<p><strong>Current VLA models</strong> represent the state of the art. <strong>RT-1 (Robotics Transformer 1)</strong> demonstrated that transformers could learn robotic control end-to-end, training on 130k demonstrations to achieve 97% success on everyday tasks. <strong>RT-2</strong> extended this by co-training on web data, enabling zero-shot generalization—it successfully manipulated objects like toy dinosaurs not present in robotic training data. <strong>PaLM-E</strong> integrated Google&#x27;s PaLM language model with robotic embodiment, creating a 562B parameter model that reasons about tasks using both robotic sensor data and language. <strong>OpenVLA</strong> provides an open-source 7B parameter model trained on the Open X-Embodiment dataset, offering strong performance with accessible compute requirements. <strong>Octo</strong> focuses on generalist policies across robot morphologies, learning action representations that transfer between different robot arms and grippers. These models establish that foundation models for robotics are viable, practical, and rapidly improving.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="92-deploying-vla-models">9.2 Deploying VLA Models<a href="#92-deploying-vla-models" class="hash-link" aria-label="Direct link to 9.2 Deploying VLA Models" title="Direct link to 9.2 Deploying VLA Models" translate="no">​</a></h2>
<p>Deploying VLA models on humanoid robots involves practical tradeoffs between model capability, computational resources, and real-time requirements. While cloud deployment offers unlimited compute for large models, edge deployment on robot hardware provides low latency and offline operation—critical for responsive manipulation and navigation in environments without reliable connectivity.</p>
<p><strong>Model selection</strong> depends on deployment constraints and task requirements. <strong>Cloud deployment</strong> suits scenarios where latency isn&#x27;t critical (planning-level tasks, infrequent decisions) or when model size exceeds edge compute capacity. Large models like PaLM-E (562B parameters) run exclusively in cloud infrastructure, requiring API calls that introduce 500ms-2s round-trip latency. This works for high-level task planning (&quot;how should I clean this room?&quot;) but fails for reactive control loops (grasping, balance correction). <strong>Edge deployment</strong> on robot hardware—NVIDIA Jetson Orin (up to 275 TOPS), embedded GPUs, or custom accelerators—enables low-latency inference (50-200ms) suitable for closed-loop control. Models like OpenVLA (7B parameters) and Octo (93M parameters) target edge deployment, fitting in 16-32GB GPU memory with optimizations. The selection criteria: if the task requires real-time feedback (manipulation, walking balance), deploy on edge; if it&#x27;s high-level reasoning tolerating latency, consider cloud. Hybrid approaches work well—cloud models for planning, edge models for execution.</p>
<p><strong>Quantization and optimization for Jetson</strong> reduce model size and increase inference speed without catastrophic accuracy loss. <strong>Quantization</strong> converts model weights from FP32 (32-bit floating point) to FP16 (16-bit) or INT8 (8-bit integers), reducing memory footprint by 2-4× and accelerating inference through specialized hardware instructions (Tensor Cores, INT8 ops). Post-training quantization (PTQ) converts trained models without retraining—tools like TensorRT (NVIDIA&#x27;s inference optimizer) handle this automatically, analyzing activation distributions to minimize quantization error. Quantization-aware training (QAT) inserts fake quantization operations during training, teaching the model to maintain accuracy under quantization. For VLA models, FP16 quantization typically loses &lt;1% accuracy and enables real-time inference on Jetson Orin; INT8 quantization achieves 3-4× speedup but may degrade performance by 2-5%, requiring validation. <strong>TensorRT optimization</strong> goes beyond quantization: it fuses layers (combining convolution+batch_norm+activation into single ops), optimizes memory layout, and generates platform-specific CUDA kernels. The workflow: export VLA model to ONNX format, convert ONNX to TensorRT engine with quantization flags, deploy the optimized engine with TensorRT runtime. This pipeline can reduce inference latency from 500ms to 100ms for a 7B parameter model on Jetson Orin.</p>
<p><strong>ROS 2 integration patterns</strong> connect VLA inference to robot perception and control. The typical architecture: a <strong>VLA inference node</strong> subscribes to camera image topics (<code>sensor_msgs/Image</code>) and language command topics (<code>std_msgs/String</code>), runs inference, and publishes action commands (<code>geometry_msgs/Twist</code> for velocity, <code>trajectory_msgs/JointTrajectory</code> for arm motion, <code>std_msgs/Bool</code> for gripper). The node manages GPU memory (pre-allocating buffers), runs inference asynchronously (to avoid blocking ROS callbacks), and handles coordinate frame transforms (converting model outputs from camera frame to robot base frame using tf2). For efficiency, the node can buffer images and run inference at a lower rate than camera publication (e.g., process every 5th frame at 6 Hz rather than every frame at 30 Hz). Action outputs often require post-processing—VLA models may output end-effector poses that need inverse kinematics to compute joint angles, or they may output relative actions (&quot;move 10cm forward&quot;) that require integration with odometry. ROS 2 action servers provide a clean interface: the VLA node can be a client to MoveIt 2 (for motion planning) or Nav2 (for navigation), translating high-level VLA decisions into robust execution.</p>
<p><strong>Latency considerations</strong> determine whether VLA models can operate in closed-loop control. Manipulation control loops run at 10-100 Hz (10-100ms cycles), requiring VLA inference latency under 50ms to leave time for sensing, control computation, and actuation. Current edge-deployed VLA models achieve 100-200ms inference latency on Jetson Orin, suitable for semi-reactive tasks (grasping stationary objects, waypoint navigation) but too slow for dynamic tasks (catching thrown objects, balancing during disturbances). Strategies to mitigate latency: <strong>temporal batching</strong> (run inference less frequently, use last action for intermediate steps), <strong>action chunking</strong> (predict sequences of future actions in one inference, execute them open-loop), <strong>hierarchical control</strong> (VLA generates high-level goals, fast low-level controller executes), and <strong>speculative execution</strong> (pre-compute likely next actions). For humanoids, a practical pattern: VLA runs at 5-10 Hz for task decisions, while 100+ Hz low-level controllers handle balance and immediate obstacle avoidance.</p>
<p><strong>Prompt engineering for robotic tasks</strong> significantly impacts VLA success rates. Unlike general chatbots, robotic VLA models benefit from structured prompts that specify task details and environmental context. Effective prompts include: <strong>object specificity</strong> (&quot;pick up the red mug with the handle&quot; vs vague &quot;pick up the mug&quot;), <strong>spatial clarity</strong> (&quot;move 20cm to the left&quot; vs &quot;move left&quot;), <strong>sequential structure</strong> for multi-step tasks (&quot;first open the drawer, then place the cup inside&quot;), and <strong>environmental context</strong> (&quot;there are two cups; grasp the one closer to you&quot;). Some VLA models support <strong>few-shot prompting</strong>—providing example instruction-action pairs before the target task to bias the model toward desired behavior. Prompt templates help: &quot;Task: [verb] the [object] [spatial_relation] | Object location: [coordinates] | Robot state: [current_pose]&quot;. Experimenting with prompt variations and logging success rates guides optimization—small wording changes can have outsized effects on model behavior.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="93-language-grounding-and-action-prediction">9.3 Language Grounding and Action Prediction<a href="#93-language-grounding-and-action-prediction" class="hash-link" aria-label="Direct link to 9.3 Language Grounding and Action Prediction" title="Direct link to 9.3 Language Grounding and Action Prediction" translate="no">​</a></h2>
<p>The defining capability of VLA models is transforming natural language instructions—inherently abstract and symbolic—into concrete robot actions grounded in the physical world. This requires understanding not just word meanings but spatial relationships, object properties, action semantics, and the robot&#x27;s embodiment constraints. Language grounding bridges the gap between human intent expressed in words and executable motor commands.</p>
<p><strong>Natural language command parsing</strong> for robotics differs from general NLP because instructions must be executable—vague language needs disambiguation, implicit assumptions need surfacing, and impossible requests need rejection. A command like &quot;bring me the cup&quot; contains several grounding challenges: which cup (if multiple exist), where is &quot;me&quot; (requires person detection and localization), what trajectory (collision-free path planning), and what constitutes success (placed within reach? handed directly?). VLA models trained on diverse robotic data learn to parse these commands by pattern matching against training examples, but parsing errors—misidentifying objects, misunderstanding verbs, ignoring spatial constraints—remain common failure modes. Robust systems combine VLA parsing with validation: after parsing &quot;move the laptop left,&quot; verify a laptop is visible, confirm &quot;left&quot; is relative to robot&#x27;s viewpoint, and check that the target location is reachable and collision-free before execution.</p>
<p><strong>Grounding language to action sequences</strong> involves mapping linguistic concepts to motor primitives. Commands like &quot;pick up&quot; ground to a sequence: approach object, open gripper, move to pre-grasp pose, close gripper, lift. &quot;Move left&quot; grounds to a base motion command with negative y-axis velocity. VLA models learn these groundings implicitly through training on demonstration data—when the model observes humans saying &quot;pick up&quot; while demonstrators execute grasp sequences, it associates the phrase with that action pattern. The challenge: compositional generalization. Can a model trained on &quot;pick up the cup&quot; and &quot;move the box left&quot; successfully execute &quot;pick up the cup and move it left&quot;? Modern VLA models exhibit some compositional understanding, but complex multi-step tasks with novel combinations often fail. This motivates hierarchical approaches: use VLA to decompose high-level commands into subtasks (&quot;pick up,&quot; &quot;move,&quot; &quot;place&quot;), then use specialized low-level controllers for each subtask, ensuring reliable execution of known primitives while leveraging VLA&#x27;s language understanding for task decomposition.</p>
<p><strong>Spatial reasoning</strong> enables understanding relational descriptions essential for manipulation and navigation. When a human says &quot;place the cup on the table,&quot; the model must: detect the cup (object grounding), detect the table (surface grounding), understand &quot;on&quot; means vertically above with contact (spatial relation), and compute a placement pose satisfying this relation. Spatial language includes <strong>locative relations</strong> (&quot;on,&quot; &quot;in,&quot; &quot;under,&quot; &quot;near&quot;), <strong>directional terms</strong> (&quot;left,&quot; &quot;right,&quot; &quot;forward,&quot; &quot;behind&quot;), <strong>distance specifications</strong> (&quot;close,&quot; &quot;far,&quot; &quot;next to&quot;), and <strong>reference frames</strong> (allocentric &quot;north of the table&quot; vs egocentric &quot;to my left&quot;). VLA models learn these through multimodal grounding—visual attention mechanisms identify relevant objects while language tokens specify their relationships. Attention maps reveal grounding: when processing &quot;left of the red cup,&quot; attention highlights red objects and regions to their left. However, spatial reasoning failures are common: models may confuse reference frames (robot&#x27;s left vs human&#x27;s left), misjudge distances, or fail with complex nested relations (&quot;on the box next to the table&quot;). Providing explicit spatial coordinates in prompts (when available) improves reliability: &quot;move to position (x=1.2, y=0.3)&quot; vs &quot;move forward a bit.&quot;</p>
<p><strong>Handling ambiguity and clarification requests</strong> distinguishes robust VLA systems from brittle ones. Ambiguous commands arise constantly: &quot;get the cup&quot; when three cups are visible, &quot;move left&quot; without specifying how far, &quot;open the door&quot; when the robot lacks door-opening capabilities. Production VLA systems should detect ambiguity—through confidence thresholds (low-confidence predictions trigger clarification), explicit ambiguity checks (object detector finds multiple matches), or feasibility validation (requested action violates constraints)—and request clarification from users. Clarification mechanisms include: <strong>object disambiguation</strong> (highlight candidates on screen or describe them: &quot;I see two cups—red one on the table, blue one on the counter. Which?&quot;), <strong>parameter elicitation</strong> (ask &quot;how far left?&quot;), <strong>capability negotiation</strong> (&quot;I cannot open doors; should I approach and wait?&quot;). Implementing clarification requires multimodal interaction: VLA generates clarification prompts, speech synthesis vocalizes them, speech recognition captures responses, and VLA processes responses to refine the command. This interactive loop transforms VLA from one-shot command execution to collaborative task completion.</p>
<p><strong>Mapping VLA outputs to ROS 2 actions</strong> requires translating model outputs—typically floating-point vectors representing poses, velocities, or gripper states—into ROS messages and action goals. VLA models output actions in various formats: <strong>end-effector deltas</strong> (relative movements: Δx, Δy, Δz, Δroll, Δpitch, Δyaw, gripper_open), <strong>absolute poses</strong> (target end-effector pose in camera or base frame), <strong>joint velocities</strong> (direct motor commands), or <strong>high-level goals</strong> (semantic actions like &quot;grasp&quot; that trigger downstream planners). The ROS integration layer converts these: end-effector poses use inverse kinematics (via MoveIt 2 or analytic solvers) to compute joint angles, deltas integrate with current odometry to compute absolute targets, and high-level goals trigger ROS action clients (e.g., a &quot;navigate to kitchen&quot; goal invokes Nav2&#x27;s NavigateToPose action). Coordinate frame transforms via tf2 are essential—VLA models typically output in camera frame, but motion planners need base frame or world frame. The integration node subscribes to VLA action topics, applies transforms, validates feasibility (collision checking, reachability), and publishes or calls the appropriate ROS 2 interfaces for execution.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="94-open-vocabulary-object-manipulation">9.4 Open-Vocabulary Object Manipulation<a href="#94-open-vocabulary-object-manipulation" class="hash-link" aria-label="Direct link to 9.4 Open-Vocabulary Object Manipulation" title="Direct link to 9.4 Open-Vocabulary Object Manipulation" translate="no">​</a></h2>
<p>Traditional robotic manipulation requires training object-specific detectors and grasp planners for each object the robot might encounter—a brittle approach that fails when novel objects appear. Open-vocabulary manipulation, enabled by VLA models, allows robots to detect, reason about, and manipulate arbitrary objects described in natural language, even if those objects never appeared in the robot&#x27;s training data. This capability transforms humanoid robots from special-purpose machines into general-purpose assistants.</p>
<p><strong>Zero-shot object detection with vision-language models</strong> eliminates the need for object-specific training. Instead of training a detector to recognize &quot;cup&quot; by showing it thousands of cup images, vision-language models like CLIP, OWL-ViT, or Grounding DINO learn joint embeddings of images and text from web-scale data, enabling them to detect any object described in text. The workflow: provide a text query (&quot;red coffee mug&quot;), the model computes text embeddings, slides a detection window across the image computing image region embeddings, and returns regions where image-text similarity exceeds a threshold—those regions contain the queried object. This approach generalizes to objects outside training data: if the model learned &quot;mug&quot; from web images and &quot;spatula&quot; from others, it can detect &quot;red spatula&quot; through compositional reasoning about color and object type. For robotics, this enables commands like &quot;pick up the yellow screwdriver&quot; to succeed even if the robot never specifically trained on yellow screwdrivers, as long as it learned &quot;yellow&quot; and &quot;screwdriver&quot; independently. The detection quality depends on distinctiveness—unique objects (unusual tools, specific brands) detect reliably, while generic objects (plain white cups among similar ones) require additional context or demonstrations.</p>
<p><strong>Object grounding in 3D space</strong> connects 2D visual detections to 3D poses required for manipulation. A detected bounding box tells you where pixels are, not where the physical object is in 3D space relative to the robot. The grounding pipeline: run open-vocabulary detection on RGB images to get 2D bounding boxes, align depth data from RGB-D cameras to find depth values within each box, project detected pixels to 3D point clouds using camera intrinsics, segment the point cloud to isolate the target object, fit geometric primitives or compute oriented bounding boxes to estimate 6D pose (position + orientation), and transform to the robot base frame via tf2. This gives the robot actionable information: &quot;the red mug is at position (0.4, -0.2, 0.8) meters from my base, oriented upright.&quot; For robust grounding, systems often fuse detections across multiple camera viewpoints—errors in one view get corrected by others, and occlusions in one view appear visible in another. Semantic segmentation (Chapter 7) enhances grounding by providing pixel-precise object boundaries rather than rectangular boxes, improving pose estimation accuracy especially for irregularly shaped objects.</p>
<p><strong>Manipulation primitives</strong> provide the building blocks VLA models compose into task solutions. Core primitives include: <strong>pick</strong> (approach, grasp, lift), <strong>place</strong> (approach target location, lower, release), <strong>push</strong> (contact object, apply force in direction), <strong>pull</strong> (grasp handle/edge, retract), <strong>pour</strong> (tilt grasped container over target), <strong>open/close</strong> (articulated object manipulation like drawers, doors), and <strong>handover</strong> (present grasped object to human). Each primitive encapsulates motion planning, force control, and failure handling—pick includes grasp pose computation (where to grip), pre-grasp approach (collision-free trajectory), grasp execution (close gripper with force limits), and lift verification (detect grasp success by monitoring gripper force/position). VLA models learn to sequence primitives: &quot;clear the table&quot; might decompose to repeated pick-and-place operations for each detected object. The open-vocabulary aspect: VLA models ground language commands to these primitives (&quot;move the stapler to the drawer&quot; → navigate-to-stapler, pick(stapler), navigate-to-drawer, open(drawer), place(stapler, in-drawer), close(drawer)) without requiring task-specific programming.</p>
<p><strong>Combining VLA with motion planning (MoveIt 2)</strong> creates a powerful hybrid: VLA handles high-level task understanding and object grounding, while MoveIt 2 ensures safe, collision-free execution. The integration pattern: VLA outputs target end-effector poses or object manipulation goals, the ROS integration layer converts these to MoveIt 2 motion planning requests (PlanningSceneInterface to represent obstacles, PlanningInterface to compute trajectories, ExecuteTrajectory action to execute), MoveIt 2 computes collision-free joint-space trajectories respecting kinematic constraints, and the robot controller executes the trajectory while monitoring for collisions or joint limits. This division of labor leverages the strengths of each component—VLA&#x27;s semantic understanding and generalization versus MoveIt 2&#x27;s geometric reasoning and safety guarantees. For instance, VLA might decide &quot;grasp the cup from above,&quot; compute a grasp pose, and pass it to MoveIt 2; MoveIt 2 verifies reachability, plans a collision-free approach avoiding the nearby laptop, and executes the motion. If MoveIt 2 reports &quot;unreachable&quot; or &quot;collision,&quot; the VLA can replan—try a different grasp angle, ask the human to move obstacles, or abort gracefully.</p>
<p><strong>Failure detection and recovery</strong> handle the inevitable errors in real-world manipulation—missed grasps, dropped objects, occluded targets, or environmental changes. VLA systems should monitor execution and detect failures through sensor feedback: grasp failure (gripper closes without resistance, indicating missed grasp), drop detection (sudden force drop during transport), localization failure (visual tracking loses object), or collision events (unexpected contact forces). Recovery strategies mirror those in navigation (Chapter 8): retry with adjusted parameters (re-grasp from different angle), use alternate primitives (push object to better position before grasping), request human assistance (ask user to hand object directly), or abort and report failure. Advanced VLA models exhibit learned recovery behaviors—if initial grasp fails, try grasping a different part of the object; if object slips, adjust grip before proceeding. The key is graceful degradation: rather than silently failing or causing damage, the system detects problems, attempts reasonable recoveries, and communicates clearly when human intervention is needed.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercises">Exercises<a href="#exercises" class="hash-link" aria-label="Direct link to Exercises" title="Direct link to Exercises" translate="no">​</a></h2>
<ol>
<li class=""><strong>VLA Inference</strong>: Deploy OpenVLA or RT-2 and test with basic manipulation commands</li>
<li class=""><strong>Language Grounding</strong>: Map commands like &quot;pick up the red cup&quot; to action sequences</li>
<li class=""><strong>Open-Vocabulary Detection</strong>: Detect arbitrary objects using VLM zero-shot capabilities</li>
<li class=""><strong>Prompt Engineering</strong>: Design prompts that improve VLA task success rates</li>
<li class=""><strong>Fine-Tuning</strong>: Collect demonstrations and fine-tune VLA on a custom task</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<ul>
<li class="">VLA models enable open-ended natural language control of robots</li>
<li class="">Pre-trained foundation models exhibit zero-shot generalization</li>
<li class="">Grounding language to actions requires spatial and semantic reasoning</li>
<li class="">Combining VLA with classical planners improves reliability</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class="">RT-2 paper: &quot;RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control&quot;</li>
<li class="">PaLM-E paper: &quot;PaLM-E: An Embodied Multimodal Language Model&quot;</li>
<li class="">OpenVLA project and model zoo</li>
<li class="">Foundation models for robotics survey papers</li>
</ul>
<hr>
<p><strong>Status</strong>: Draft complete (2,340 words)</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book/tree/main/book/docs/09-vla-models.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">8. Navigation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">10. Voice to Action</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#91-the-vla-paradigm" class="table-of-contents__link toc-highlight">9.1 The VLA Paradigm</a></li><li><a href="#92-deploying-vla-models" class="table-of-contents__link toc-highlight">9.2 Deploying VLA Models</a></li><li><a href="#93-language-grounding-and-action-prediction" class="table-of-contents__link toc-highlight">9.3 Language Grounding and Action Prediction</a></li><li><a href="#94-open-vocabulary-object-manipulation" class="table-of-contents__link toc-highlight">9.4 Open-Vocabulary Object Manipulation</a></li><li><a href="#exercises" class="table-of-contents__link toc-highlight">Exercises</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone">Capstone Project</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.omniverse.nvidia.com/isaacsim/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI Robotics Team. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>