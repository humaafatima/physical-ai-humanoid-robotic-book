<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-13-capstone" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 13 - Capstone Project - Autonomous Conversational Humanoid | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 13 - Capstone Project - Autonomous Conversational Humanoid | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="The culminating project that integrates ROS 2, Isaac Sim, perception, navigation, manipulation, and voice control into a complete conversational humanoid robot."><meta data-rh="true" property="og:description" content="The culminating project that integrates ROS 2, Isaac Sim, perception, navigation, manipulation, and voice control into a complete conversational humanoid robot."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone" hreflang="en"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"13. Capstone Project","item":"https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="Physical AI &amp; Humanoid Robotics" href="/Physical-AI-Humanoid-Robotics-Book/opensearch.xml"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Book/assets/css/styles.03ae9914.css">
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/runtime~main.d422561b.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/main.e7e20e78.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Book/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Book/docs/">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/"><span title="Welcome" class="linkLabel_WmDU">Welcome</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro"><span title="1. Physical AI &amp; Embodied Intelligence" class="linkLabel_WmDU">1. Physical AI &amp; Embodied Intelligence</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/02-landscape"><span title="2. Humanoid Robotics Landscape" class="linkLabel_WmDU">2. Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/03-ros2"><span title="3. ROS 2" class="linkLabel_WmDU">3. ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/04-urdf"><span title="4. URDF &amp; Robot Modeling" class="linkLabel_WmDU">4. URDF &amp; Robot Modeling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/05-digital-twins"><span title="5. Digital Twins" class="linkLabel_WmDU">5. Digital Twins</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/06-sensors"><span title="6. Sensors" class="linkLabel_WmDU">6. Sensors</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/07-isaac-ros"><span title="7. Isaac ROS" class="linkLabel_WmDU">7. Isaac ROS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><span title="8. Navigation" class="linkLabel_WmDU">8. Navigation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><span title="9. VLA Models" class="linkLabel_WmDU">9. VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><span title="10. Voice to Action" class="linkLabel_WmDU">10. Voice to Action</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><span title="11. Manipulation" class="linkLabel_WmDU">11. Manipulation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><span title="12. Sim-to-Real" class="linkLabel_WmDU">12. Sim-to-Real</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><span title="13. Capstone Project" class="linkLabel_WmDU">13. Capstone Project</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware"><span title="14. Hardware" class="linkLabel_WmDU">14. Hardware</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">13. Capstone Project</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 13: Capstone Project – Autonomous Conversational Humanoid</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Chapter Overview</div><div class="admonitionContent_BuS1"><p>The culminating project that integrates ROS 2, Isaac Sim, perception, navigation, manipulation, and voice control into a complete conversational humanoid robot.</p><p><strong>Word Target</strong>: 2,000-2,500 words (longest chapter)
<strong>Code Example</strong>: 1 large integrated project + starter code</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<ul>
<li class="">Design and implement a complete robotic system architecture</li>
<li class="">Integrate perception, navigation, manipulation, and voice control</li>
<li class="">Debug complex multi-component failures</li>
<li class="">Evaluate system performance against requirements</li>
<li class="">Document and present your work</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="131-project-overview">13.1 Project Overview<a href="#131-project-overview" class="hash-link" aria-label="Direct link to 13.1 Project Overview" title="Direct link to 13.1 Project Overview" translate="no">​</a></h2>
<p>The capstone project synthesizes every concept from the preceding twelve chapters into a single integrated system: an <strong>autonomous conversational humanoid robot</strong> that understands natural language commands, navigates complex environments, manipulates objects, and provides spoken feedback. This is not a toy demonstration—it&#x27;s a production-quality system requiring careful architecture, robust error handling, and thorough testing. You&#x27;ll experience the full software development lifecycle: requirements analysis, system design, component integration, debugging distributed failures, performance optimization, and validation against success criteria. By the end, you&#x27;ll have built a complete humanoid robotics application from scratch and gained the practical systems engineering skills necessary for real-world robotics development.</p>
<p><strong>Problem Statement</strong>: Develop a humanoid robot that can autonomously complete household assistance tasks specified via voice commands. The robot must navigate through indoor environments with obstacles (furniture, doorways, narrow passages), locate and grasp target objects, and deliver them to specified locations. The system must handle uncertainties—objects might be occluded or misidentified, paths might be blocked requiring replanning, grasps might fail requiring retries—gracefully recovering from failures rather than catastrophically aborting. The user interface is entirely voice-based: users speak natural language commands (&quot;bring me the red cup from the kitchen table&quot;), the robot acknowledges understanding, executes the task while providing progress updates, and confirms completion verbally.</p>
<p><strong>Core Requirements</strong> establish measurable success criteria: <strong>Navigation</strong> using Nav2 must plan and execute collision-free paths through cluttered environments with dynamic replanning when obstacles are detected; <strong>Manipulation</strong> via MoveIt 2 must achieve 90%+ grasp success on cylindrical objects (cups, bottles) and 70%+ on irregular objects (books, tools); <strong>Voice Interface</strong> must transcribe user commands with &lt;5% word error rate and generate executable action plans for a vocabulary of 20+ command variations; <strong>Perception</strong> using Isaac ROS must detect and localize target objects with 85%+ precision within 5 meters; <strong>System Integration</strong> requires all components operational simultaneously with end-to-end latency &lt;2 seconds from command recognition to motion execution start. The system must successfully complete three demonstration scenarios (detailed below) within a 10-minute session each, with at most one retry per scenario due to recoverable failures.</p>
<p><strong>Timeline and Milestones</strong>: Allocate 2-3 weeks for full implementation. Week 1: Set up infrastructure (Isaac Sim environment, robot URDF configuration, ROS 2 workspace), implement and test individual components (perception node, navigation node, manipulation node, voice interface) in isolation with unit tests. Week 2: Integrate component pairs (perception + navigation, navigation + manipulation, voice + task planning), debug inter-component failures (timing issues, message format mismatches, state synchronization), develop state machine orchestrating complete task execution. Week 3: End-to-end system testing with demonstration scenarios, performance profiling and optimization, failure mode analysis and recovery implementation, documentation and video preparation. If hardware is available, reserve Week 3&#x27;s final days for sim-to-real transfer (Chapter 12 techniques) and physical deployment validation.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="132-system-architecture">13.2 System Architecture<a href="#132-system-architecture" class="hash-link" aria-label="Direct link to 13.2 System Architecture" title="Direct link to 13.2 System Architecture" translate="no">​</a></h2>
<p>The system follows a layered architecture with clear separation of concerns, enabling independent development and testing of components while facilitating systematic integration. The architecture comprises five layers: <strong>Simulation &amp; Hardware</strong> (Isaac Sim or physical robot), <strong>Sensing</strong> (raw sensor data acquisition), <strong>Perception</strong> (sensor processing and world modeling), <strong>Planning</strong> (task decomposition and motion planning), and <strong>Execution</strong> (action servers and state orchestration). Communication between layers uses ROS 2 topics for sensor streams, services for synchronous queries, and actions for long-running behaviors. This design allows components to be replaced or upgraded independently—for example, swapping the perception node from Isaac ROS to a custom vision pipeline without affecting navigation or manipulation.</p>
<p><strong>Core Components and Data Flow</strong>: The <strong>Voice Interface Node</strong> subscribes to microphone audio (<code>/audio/input</code>), runs Whisper transcription, and publishes text to <code>/voice/transcription</code>. The <strong>Task Planner Node</strong> receives transcriptions, queries the LLM to generate structured action sequences (JSON), validates them against robot capabilities, and publishes task plans to <code>/task_plan</code>. The <strong>State Machine Node</strong> acts as the central orchestrator: it subscribes to <code>/task_plan</code>, decomposes tasks into atomic actions, calls appropriate action servers (navigation, manipulation, speech), monitors execution status, handles failures with retries or alternative plans, and publishes system state to <code>/robot_state</code>. The <strong>Perception Node</strong> processes camera (<code>/camera/image_raw</code>) and depth (<code>/camera/depth</code>) streams through Isaac ROS pipelines, detecting objects and publishing their 6D poses to <code>/detected_objects</code>. The <strong>VSLAM Node</strong> fuses camera and IMU (<code>/imu/data</code>) for localization, publishing odometry to <code>/odom</code> and maintaining a map on <code>/map</code>. The <strong>Navigation Node</strong> wraps Nav2: it provides a <code>NavigateToGoal</code> action server, receives target poses, plans paths using costmaps (fed by <code>/scan</code> from LiDAR and <code>/detected_objects</code> for dynamic obstacles), and commands base velocity via <code>/cmd_vel</code>. The <strong>Manipulation Node</strong> wraps MoveIt 2: it provides <code>GraspObject</code> and <code>PlaceObject</code> action servers, receives object poses, plans arm trajectories, and commands joint positions via <code>/joint_trajectory_controller/joint_trajectory</code>.</p>
<p><strong>Failure Modes and Recovery</strong>: Every component includes failure detection and recovery mechanisms. <strong>Perception failures</strong> (object not detected, pose estimate has low confidence) trigger active sensing: the robot adjusts camera angle, moves closer, or queries &quot;I cannot see the red cup clearly. Is it on the table?&quot; <strong>Navigation failures</strong> (path blocked, goal unreachable) invoke replanning: update costmap with new obstacle information, replan with different planner (switch from RRT to informed RRT*), or request user guidance &quot;The path to the kitchen is blocked. Should I go through the living room?&quot; <strong>Manipulation failures</strong> (grasp fails, object slips) retry with alternative grasp poses (if multiple candidates were scored during grasp planning), reduce gripper force and retry (for fragile objects), or escalate to user &quot;I tried three times but cannot grasp the book. Please reposition it.&quot; The state machine maintains a <strong>failure counter</strong> per action type—after three consecutive failures of the same type, it transitions to a <strong>safe state</strong> (return to home position, verbally report issue, await user instruction) rather than exhausting retries indefinitely. All failures are logged with timestamps, sensor data snapshots, and recovery actions attempted, enabling offline analysis to improve future robustness.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="133-integration-checklist">13.3 Integration Checklist<a href="#133-integration-checklist" class="hash-link" aria-label="Direct link to 13.3 Integration Checklist" title="Direct link to 13.3 Integration Checklist" translate="no">​</a></h2>
<p>Before attempting end-to-end demonstration scenarios, validate each subsystem independently to isolate integration failures from component bugs. Work through this checklist systematically—attempting full-system integration with broken components wastes debugging time chasing symptoms rather than root causes.</p>
<p><strong>Simulation and Sensors</strong>: Launch Isaac Sim with your humanoid URDF and test environment. Verify: robot loads without kinematic errors (check RViz joint state visualization), all sensors publish at expected rates (<code>ros2 topic hz /camera/image_raw</code> should show 30 Hz), camera images display correctly in RViz ImageView, depth images show reasonable values (not all zeros or infinities), IMU publishes orientation and angular velocity without excessive noise. If any sensor fails, check Isaac Sim sensor configurations in the USD file and ROS 2 bridge settings.</p>
<p><strong>Perception Pipeline</strong>: Run Isaac ROS nodes. Verify: object detection publishes bounding boxes to <code>/detected_objects</code> (visualize in RViz with MarkerArray), detection confidence exceeds 0.8 for target objects, pose estimates are stable (object positions don&#x27;t jitter &gt;5cm frame-to-frame). Test with objects at various distances (1-5 meters) and orientations. If detection fails, tune confidence thresholds, verify camera intrinsics calibration, check lighting conditions in simulation.</p>
<p><strong>Localization</strong>: Launch VSLAM (ORB-SLAM3 or Isaac ROS Visual SLAM). Verify: odometry publishes to <code>/odom</code>, map builds correctly (visualize in RViz), localization error &lt;10cm compared to ground truth (Isaac Sim provides perfect ground truth via <code>/ground_truth/odom</code>). Move robot in simulation and confirm map updates. If localization drifts, tune VSLAM parameters or add more visual features to environment.</p>
<p><strong>Navigation</strong>: Launch Nav2 with configured costmaps and planners. Verify: global costmap displays obstacles from LiDAR/depth, local costmap updates dynamically, path planning succeeds for reachable goals (send test goals via RViz), path execution follows planned trajectory within 20cm tolerance, robot avoids obstacles dynamically (place movable obstacle in path during execution). If planning fails, check costmap inflation radius, planner timeout settings, and controller tracking tolerances.</p>
<p><strong>Manipulation</strong>: Launch MoveIt 2. Verify: planning scene shows robot and environment (visualize in MoveIt RViz plugin), motion planning succeeds for reachable poses (test with interactive markers), trajectory execution moves joints to target configurations, collision checking prevents invalid plans (plan motion toward obstacle, verify it&#x27;s rejected). Test grasp execution on sample object: detect object, plan grasp approach, execute grasp, verify object is grasped (check gripper sensor feedback or visual confirmation).</p>
<p><strong>Voice Interface</strong>: Run Whisper node with microphone or test audio files. Verify: transcriptions appear on <code>/voice/transcription</code> topic, transcription accuracy &gt;95% for clear speech, latency from audio to text &lt;1 second. Test with command variations: &quot;bring me the cup&quot;, &quot;get the red cup&quot;, &quot;fetch the cup from table&quot;. If accuracy is low, check microphone quality, add noise filtering, or use larger Whisper model.</p>
<p><strong>Task Planning</strong>: Feed sample commands to LLM. Verify: LLM generates valid JSON action sequences, all action types in output match available action servers (navigate, grasp, place, speak), spatial references are resolved (&quot;kitchen&quot; maps to coordinates, &quot;red cup&quot; specifies object class and attribute). If invalid plans are generated, refine LLM system prompt with examples and constraints.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="134-step-by-step-implementation-guide">13.4 Step-by-Step Implementation Guide<a href="#134-step-by-step-implementation-guide" class="hash-link" aria-label="Direct link to 13.4 Step-by-Step Implementation Guide" title="Direct link to 13.4 Step-by-Step Implementation Guide" translate="no">​</a></h2>
<p><strong>(1) Environment Setup</strong>: Create an Isaac Sim scene representing a home environment. Add floor plane, walls forming 3-4 rooms (living room, kitchen, bedroom), furniture (tables, chairs, shelves), and target objects (cups, books, tools). Place obstacles requiring navigation around them. Save as <code>capstone_environment.usd</code>. Configure lighting to avoid extreme shadows that confuse perception.</p>
<p><strong>(2) Robot Configuration</strong>: Use a humanoid URDF (modify existing template or create from CAD). Add sensors: RGB-D camera on head (<code>resolution: 640x480, FOV: 90°, depth range: 0.5-10m</code>), IMU on torso, LiDAR on base for navigation. Configure joint controllers in <code>ros2_control</code>: velocity controllers for wheels, position controllers for arms. Validate URDF loads in Isaac Sim without errors.</p>
<p><strong>(3) Perception Pipeline</strong>: Launch Isaac ROS object detection (<code>isaac_ros_detectnet</code> or <code>isaac_ros_yolov8</code>). Train or load pre-trained model on target objects. Create perception node that: subscribes to detections, filters by confidence &gt;0.8, estimates 6D poses using depth, publishes to <code>/detected_objects</code> as <code>vision_msgs/Detection3DArray</code>. Test with objects at various locations.</p>
<p><strong>(4) Localization and Mapping</strong>: Launch Isaac ROS Visual SLAM. Configure with camera topics and IMU. Initialize in known starting pose. Drive robot through environment to build map. Save map for reuse (<code>ros2 run nav2_map_server map_saver_cli</code>). Verify localization by comparing <code>/odom</code> to ground truth.</p>
<p><strong>(5) Navigation Setup</strong>: Configure Nav2 with <code>nav2_params.yaml</code>: set costmap parameters (global: 50m x 50m, local: 5m x 5m, inflation radius: 0.3m), configure planners (global: NavFn or Smac Planner, local: DWB), set controller parameters for differential drive or omnidirectional base. Launch Nav2 stack and test with 2D Nav Goal in RViz.</p>
<p><strong>(6) Manipulation Setup</strong>: Run MoveIt Setup Assistant on your URDF to generate MoveIt config package. Define planning groups (<code>right_arm</code>, <code>left_arm</code>), set end-effector links, configure collision matrix. Install TracIK for inverse kinematics. Launch MoveIt and test motion planning with interactive markers in RViz. Create <code>manipulation_node.py</code> with action servers for <code>GraspObject(object_id)</code> and <code>PlaceObject(target_pose)</code>.</p>
<p><strong>(7) Voice Interface</strong>: Create <code>voice_interface_node.py</code>. Initialize Whisper model (<code>whisper.load_model(&quot;base&quot;)</code>). Subscribe to <code>/audio/input</code>, run inference on 30-second chunks, publish transcriptions to <code>/voice/transcription</code>. For testing without microphone, create script that publishes pre-recorded audio or text directly.</p>
<p><strong>(8) Task Planner</strong>: Create <code>task_planner_node.py</code>. Subscribe to <code>/voice/transcription</code>. On new command: send to LLM with system prompt defining available actions and JSON schema, parse LLM response, validate action types and parameters, publish to <code>/task_plan</code>. Implement local fallback for common commands if LLM unavailable.</p>
<p><strong>(9) State Machine</strong>: Create <code>state_machine_node.py</code> using SMACH or custom finite state machine. States: <code>IDLE</code> (waiting for command), <code>PLANNING</code> (receiving task plan), <code>EXECUTING</code> (calling action servers), <code>RECOVERING</code> (handling failures), <code>COMPLETED</code> (task done). Implement action client for each action type (navigate, grasp, place). Add failure counters and retry logic.</p>
<p><strong>(10) Action Servers</strong>: Implement ROS 2 action servers for atomic behaviors: <code>NavigateToGoal.action</code> (wraps Nav2), <code>GraspObject.action</code> (perception + MoveIt grasp), <code>PlaceObject.action</code> (MoveIt place + gripper open), <code>Speak.action</code> (text-to-speech for user feedback). Each server reports progress and handles cancellation.</p>
<p><strong>(11) Integration</strong>: Create <code>full_system.launch.py</code> that launches all nodes in correct order with dependencies. Use launch events to ensure Isaac Sim is ready before launching ROS nodes. Add parameter configurations and remappings. Test launch file brings up entire system.</p>
<p><strong>(12) End-to-End Testing</strong>: Run demonstration scenarios (defined below). Use RViz and Foxglove for visualization. Log all topics with <code>ros2 bag record</code>. Profile with <code>ros2 topic hz</code> and <code>ros2 node list</code> to verify all components running. Debug failures using logged data replay.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="135-testing-and-debugging">13.5 Testing and Debugging<a href="#135-testing-and-debugging" class="hash-link" aria-label="Direct link to 13.5 Testing and Debugging" title="Direct link to 13.5 Testing and Debugging" translate="no">​</a></h2>
<p><strong>Unit Testing</strong>: Test each node in isolation with mock inputs. For perception node: publish recorded camera images, verify detection output format and confidence scores. For navigation node: send test goal poses, verify path planning succeeds without executing motion. For manipulation node: provide known object poses, verify grasp poses are computed correctly. Use <code>pytest</code> with ROS 2 test framework for automated unit tests. Mock external dependencies (LLM API, hardware interfaces) to ensure deterministic behavior.</p>
<p><strong>Integration Testing</strong>: Test component pairs. Perception + Navigation: detect object, navigate to detected pose, verify robot reaches correct location. Navigation + Manipulation: navigate to table, execute grasp on object, verify end-to-end completion. Use integration test scenarios with known initial states and expected outcomes. Record successful runs as regression tests—future code changes should not break previously working integration.</p>
<p><strong>System Testing</strong>: Run complete demonstration scenarios. For each scenario: record initial conditions (robot pose, object locations), execute voice command, log all ROS topics, verify success criteria (task completed, object grasped, navigation succeeded). Repeat each scenario 10 times to measure success rate—target is 90% success with at most one retry. Identify failure modes: perception misidentification (8% of failures), navigation replanning needed (5%), grasp failures requiring retry (7%). Address most common failures first for maximum impact.</p>
<p><strong>Common Failures and Solutions</strong>: <strong>(1) Timing issues</strong>: Nodes start before Isaac Sim ready → add delays in launch file or poll for topic availability. <strong>(2) Transform errors</strong>: TF tree incomplete → verify all required transforms published, check <code>ros2 run tf2_tools view_frames.py</code>. <strong>(3) Action server timeouts</strong>: Long-running actions exceed default timeout → increase action client timeout or add progress feedback. <strong>(4) Perception failures</strong>: False positives/negatives → tune confidence thresholds, improve lighting, add more training data. <strong>(5) Navigation oscillations</strong>: Robot vibrates near goal → tune DWB controller gains, reduce velocity limits. <strong>(6) MoveIt planning failures</strong>: &quot;No solution found&quot; → increase planning time, use different planner, simplify scene.</p>
<p><strong>Debugging Tools</strong>: Use <code>rqt_graph</code> to visualize node connections and verify expected data flow. Use <code>rqt_console</code> to view log messages with severity filtering (DEBUG, INFO, WARN, ERROR). Record failure cases with <code>ros2 bag record -a</code>, replay with <code>ros2 bag play</code> for offline debugging. Visualize in RViz: TF frames, camera images, detected objects (MarkerArray), costmaps, planned paths. For complex multi-modal debugging, use Foxglove Studio—visualize multiple topics simultaneously, scrub through recorded data, create custom visualization panels. Profile CPU/memory with <code>ros2 run ros2_performance performance_test</code>—identify bottleneck nodes consuming excessive resources.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="136-evaluation-rubric">13.6 Evaluation Rubric<a href="#136-evaluation-rubric" class="hash-link" aria-label="Direct link to 13.6 Evaluation Rubric" title="Direct link to 13.6 Evaluation Rubric" translate="no">​</a></h2>
<p>Your capstone project is evaluated holistically on functionality, integration, robustness, and engineering quality. This rubric guides both self-assessment during development and final evaluation.</p>
<p><strong>Functionality (40%)</strong>: Does the system complete the three required demonstration scenarios? <strong>(15 points)</strong> Successfully completes all three scenarios end-to-end on first attempt. <strong>(12 points)</strong> Completes all three with one retry per scenario allowed. <strong>(9 points)</strong> Completes two scenarios consistently, third succeeds 50% of time. <strong>(6 points)</strong> Completes one scenario reliably, others partially functional. <strong>(0-5 points)</strong> Major functionality missing or non-operational.</p>
<p><strong>Integration (20%)</strong>: Do all components work together as a cohesive system? <strong>(20 points)</strong> All subsystems communicate correctly, state transitions smoothly, data flows as designed, no manual intervention required. <strong>(15 points)</strong> Minor integration issues (occasional message drops, timing glitches) that self-recover. <strong>(10 points)</strong> Integration requires occasional manual reset or component restart. <strong>(5 points)</strong> Components work independently but integration unstable. <strong>(0 points)</strong> Components do not integrate.</p>
<p><strong>Robustness (15%)</strong>: Does the system handle failures gracefully? <strong>(15 points)</strong> Detects all tested failure modes (object occlusion, navigation blockage, grasp failure), recovers automatically or requests user assistance appropriately, never crashes. <strong>(11 points)</strong> Handles most failures, rare crashes with clear error messages. <strong>(7 points)</strong> Handles common failures, crashes on edge cases. <strong>(3 points)</strong> Minimal error handling, frequently crashes. <strong>(0 points)</strong> No error handling.</p>
<p><strong>Code Quality (10%)</strong>: Is code maintainable and follows best practices? <strong>(10 points)</strong> Clean, modular, well-documented (docstrings, inline comments), follows ROS 2 naming conventions, type hints used, passes linters (ruff, pylint). <strong>(7 points)</strong> Generally clean, some documentation, mostly follows conventions. <strong>(4 points)</strong> Functional but messy, minimal documentation. <strong>(0 points)</strong> Unreadable, no documentation.</p>
<p><strong>Documentation (10%)</strong>: Is the project understandable to others? <strong>(10 points)</strong> Comprehensive README with architecture diagram, setup instructions, API documentation, troubleshooting guide, demo video. <strong>(7 points)</strong> Good README, adequate diagrams, demo video. <strong>(4 points)</strong> Basic README, minimal supplementary docs. <strong>(0 points)</strong> Missing or inadequate documentation.</p>
<p><strong>Presentation (5%)</strong>: Can you clearly explain your design? <strong>(5 points)</strong> Clear 10-minute presentation covering architecture, key design decisions, challenges faced, solutions implemented, with visual aids. <strong>(3 points)</strong> Adequate presentation, some unclear explanations. <strong>(0 points)</strong> Poor presentation or unable to explain design rationale.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="demonstration-scenarios">Demonstration Scenarios<a href="#demonstration-scenarios" class="hash-link" aria-label="Direct link to Demonstration Scenarios" title="Direct link to Demonstration Scenarios" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-1-navigation-and-object-fetching">Scenario 1: Navigation and Object Fetching<a href="#scenario-1-navigation-and-object-fetching" class="hash-link" aria-label="Direct link to Scenario 1: Navigation and Object Fetching" title="Direct link to Scenario 1: Navigation and Object Fetching" translate="no">​</a></h3>
<p><strong>Command</strong>: &quot;Go to the kitchen and bring me the red cup&quot;
<strong>Requirements</strong>:</p>
<ul>
<li class="">Navigate to kitchen (predefined location)</li>
<li class="">Identify red cup using object detection</li>
<li class="">Navigate to cup location</li>
<li class="">Grasp cup with manipulator</li>
<li class="">Return to user location</li>
<li class="">Place cup in front of user</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-2-assisted-task-execution">Scenario 2: Assisted Task Execution<a href="#scenario-2-assisted-task-execution" class="hash-link" aria-label="Direct link to Scenario 2: Assisted Task Execution" title="Direct link to Scenario 2: Assisted Task Execution" translate="no">​</a></h3>
<p><strong>Command</strong>: &quot;Help me clean the table&quot;
<strong>Requirements</strong>:</p>
<ul>
<li class="">Navigate to table</li>
<li class="">Identify objects on table (cups, plates, utensils)</li>
<li class="">Pick up each object</li>
<li class="">Navigate to designated drop-off location</li>
<li class="">Place object</li>
<li class="">Repeat until table is clear</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-3-multi-step-planning">Scenario 3: Multi-Step Planning<a href="#scenario-3-multi-step-planning" class="hash-link" aria-label="Direct link to Scenario 3: Multi-Step Planning" title="Direct link to Scenario 3: Multi-Step Planning" translate="no">​</a></h3>
<p><strong>Command</strong>: &quot;Get the book from the shelf and place it on the desk&quot;
<strong>Requirements</strong>:</p>
<ul>
<li class="">Navigate to bookshelf</li>
<li class="">Identify target book (by color or text recognition)</li>
<li class="">Reach and grasp book</li>
<li class="">Navigate to desk (avoiding obstacles)</li>
<li class="">Place book on desk in upright position</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="code-example-reference-implementation">Code Example: Reference Implementation<a href="#code-example-reference-implementation" class="hash-link" aria-label="Direct link to Code Example: Reference Implementation" title="Direct link to Code Example: Reference Implementation" translate="no">​</a></h2>
<p><strong>Directory</strong>: <code>code-examples/chapter-13-capstone/reference_impl/</code></p>
<p><strong>Structure</strong>:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">reference_impl/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── config/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── nav2_params.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── moveit_config/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── isaac_sim_scene.usd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── launch/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── full_system.launch.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── src/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── perception_node.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── navigation_node.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── manipulation_node.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── voice_interface_node.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── task_planner_node.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── state_machine_node.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── Dockerfile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── docker-compose.yml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└── README.md</span><br></span></code></pre></div></div>
<p><strong>Starter Code</strong>: <code>code-examples/chapter-13-capstone/starter_code/</code></p>
<ul>
<li class="">Provides skeleton with TODO comments</li>
<li class="">Students fill in implementation details</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercises">Exercises<a href="#exercises" class="hash-link" aria-label="Direct link to Exercises" title="Direct link to Exercises" translate="no">​</a></h2>
<ol>
<li class="">
<p><strong>Component Testing</strong>: Write unit tests for each major node (perception, navigation, manipulation)</p>
</li>
<li class="">
<p><strong>Failure Injection</strong>: Simulate sensor failures (camera dropout, network delay) and implement recovery</p>
</li>
<li class="">
<p><strong>Performance Optimization</strong>: Profile your system and identify bottlenecks. Optimize to achieve real-time performance.</p>
</li>
<li class="">
<p><strong>Additional Scenarios</strong>: Design and implement 2 additional demonstration scenarios beyond the 3 required.</p>
</li>
<li class="">
<p><strong>Sim-to-Real</strong>: If hardware available, deploy one scenario to physical robot (Chapter 14 required)</p>
</li>
<li class="">
<p><strong>Multi-Robot</strong>: Extend to coordinate 2 humanoids for collaborative task</p>
</li>
<li class="">
<p><strong>Learning Component</strong>: Add reinforcement learning for grasp optimization</p>
</li>
<li class="">
<p><strong>User Study</strong>: Have 5 people test your system with novel voice commands, measure success rate</p>
</li>
<li class="">
<p><strong>Safety Features</strong>: Implement emergency stop, collision avoidance, soft limits on joint velocities</p>
</li>
<li class="">
<p><strong>Documentation</strong>: Write complete technical documentation including architecture diagrams, API references, and deployment guide</p>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="submission-requirements">Submission Requirements<a href="#submission-requirements" class="hash-link" aria-label="Direct link to Submission Requirements" title="Direct link to Submission Requirements" translate="no">​</a></h2>
<ul>
<li class=""><strong>Code</strong>: Complete ROS 2 workspace with all nodes</li>
<li class=""><strong>Docker</strong>: Dockerfile and docker-compose for reproducible setup</li>
<li class=""><strong>Documentation</strong>: README with architecture, setup instructions, API docs</li>
<li class=""><strong>Video</strong>: 5-minute demo video showing all 3 scenarios</li>
<li class=""><strong>Report</strong>: 2-3 page technical report explaining design decisions, challenges, and solutions</li>
<li class=""><strong>Presentation</strong>: 10-minute presentation to class/instructor</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-chapter-chapter-14-hardware-guide-"><strong>Next Chapter</strong>: <a class="" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware">Chapter 14: Hardware Guide →</a><a href="#next-chapter-chapter-14-hardware-guide-" class="hash-link" aria-label="Direct link to next-chapter-chapter-14-hardware-guide-" title="Direct link to next-chapter-chapter-14-hardware-guide-" translate="no">​</a></h2>
<p><strong>Status</strong>: ✅ Content complete (2,350 words) - Phase 13 drafted 2025-12-13</p>
<p>This chapter integrates concepts from all previous chapters into a complete system. It serves as the culminating project demonstrating mastery of humanoid robotics development.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book/tree/main/book/docs/13-capstone.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">12. Sim-to-Real</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">14. Hardware</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#131-project-overview" class="table-of-contents__link toc-highlight">13.1 Project Overview</a></li><li><a href="#132-system-architecture" class="table-of-contents__link toc-highlight">13.2 System Architecture</a></li><li><a href="#133-integration-checklist" class="table-of-contents__link toc-highlight">13.3 Integration Checklist</a></li><li><a href="#134-step-by-step-implementation-guide" class="table-of-contents__link toc-highlight">13.4 Step-by-Step Implementation Guide</a></li><li><a href="#135-testing-and-debugging" class="table-of-contents__link toc-highlight">13.5 Testing and Debugging</a></li><li><a href="#136-evaluation-rubric" class="table-of-contents__link toc-highlight">13.6 Evaluation Rubric</a></li><li><a href="#demonstration-scenarios" class="table-of-contents__link toc-highlight">Demonstration Scenarios</a><ul><li><a href="#scenario-1-navigation-and-object-fetching" class="table-of-contents__link toc-highlight">Scenario 1: Navigation and Object Fetching</a></li><li><a href="#scenario-2-assisted-task-execution" class="table-of-contents__link toc-highlight">Scenario 2: Assisted Task Execution</a></li><li><a href="#scenario-3-multi-step-planning" class="table-of-contents__link toc-highlight">Scenario 3: Multi-Step Planning</a></li></ul></li><li><a href="#code-example-reference-implementation" class="table-of-contents__link toc-highlight">Code Example: Reference Implementation</a></li><li><a href="#exercises" class="table-of-contents__link toc-highlight">Exercises</a></li><li><a href="#submission-requirements" class="table-of-contents__link toc-highlight">Submission Requirements</a></li><li><a href="#next-chapter-chapter-14-hardware-guide-" class="table-of-contents__link toc-highlight"><strong>Next Chapter</strong>: Chapter 14: Hardware Guide →</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone">Capstone Project</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.omniverse.nvidia.com/isaacsim/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI Robotics Team. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>