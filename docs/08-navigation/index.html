<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-08-navigation" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 8 - Autonomous Navigation with Nav2 | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 8 - Autonomous Navigation with Nav2 | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Complete guide to autonomous navigation for humanoid robots using ROS 2 Nav2 stack."><meta data-rh="true" property="og:description" content="Complete guide to autonomous navigation for humanoid robots using ROS 2 Nav2 stack."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation" hreflang="en"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"8. Navigation","item":"https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="Physical AI &amp; Humanoid Robotics" href="/Physical-AI-Humanoid-Robotics-Book/opensearch.xml"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Book/assets/css/styles.03ae9914.css">
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/runtime~main.d422561b.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/main.e7e20e78.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Book/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Book/docs/">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/"><span title="Welcome" class="linkLabel_WmDU">Welcome</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro"><span title="1. Physical AI &amp; Embodied Intelligence" class="linkLabel_WmDU">1. Physical AI &amp; Embodied Intelligence</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/02-landscape"><span title="2. Humanoid Robotics Landscape" class="linkLabel_WmDU">2. Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/03-ros2"><span title="3. ROS 2" class="linkLabel_WmDU">3. ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/04-urdf"><span title="4. URDF &amp; Robot Modeling" class="linkLabel_WmDU">4. URDF &amp; Robot Modeling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/05-digital-twins"><span title="5. Digital Twins" class="linkLabel_WmDU">5. Digital Twins</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/06-sensors"><span title="6. Sensors" class="linkLabel_WmDU">6. Sensors</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/07-isaac-ros"><span title="7. Isaac ROS" class="linkLabel_WmDU">7. Isaac ROS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><span title="8. Navigation" class="linkLabel_WmDU">8. Navigation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><span title="9. VLA Models" class="linkLabel_WmDU">9. VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><span title="10. Voice to Action" class="linkLabel_WmDU">10. Voice to Action</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><span title="11. Manipulation" class="linkLabel_WmDU">11. Manipulation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><span title="12. Sim-to-Real" class="linkLabel_WmDU">12. Sim-to-Real</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><span title="13. Capstone Project" class="linkLabel_WmDU">13. Capstone Project</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware"><span title="14. Hardware" class="linkLabel_WmDU">14. Hardware</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">8. Navigation</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 8: Autonomous Navigation with Nav2</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Chapter Overview</div><div class="admonitionContent_BuS1"><p>Complete guide to autonomous navigation for humanoid robots using ROS 2 Nav2 stack.</p><p><strong>Word Target</strong>: 1,700-1,900 words
<strong>Code Examples</strong>: 5 (SLAM, localization, path planning, behavior trees, dynamic obstacles)</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Configure Nav2 for humanoid bipedal locomotion</li>
<li class="">Perform SLAM (Simultaneous Localization and Mapping)</li>
<li class="">Implement global and local path planning</li>
<li class="">Design custom navigation behavior trees</li>
<li class="">Handle dynamic obstacles and recovery scenarios</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="81-nav2-architecture">8.1 Nav2 Architecture<a href="#81-nav2-architecture" class="hash-link" aria-label="Direct link to 8.1 Nav2 Architecture" title="Direct link to 8.1 Nav2 Architecture" translate="no">​</a></h2>
<p>Autonomous navigation represents one of the most complex capabilities a humanoid robot must possess—combining perception, localization, planning, and control into a real-time system that safely moves through dynamic environments. ROS 2 Navigation (Nav2) provides a production-ready navigation stack that replaces ROS 1&#x27;s navigation system with significant architectural improvements: lifecycle node management, behavior trees for flexible decision-making, and improved plugin interfaces for algorithm customization.</p>
<p><strong>Nav2 component architecture</strong> decomposes navigation into modular servers that communicate via ROS 2 actions and services. The <strong>BT Navigator</strong> serves as the orchestrator, executing behavior trees that define navigation logic—when to plan, when to replan, when to recover from failures. The <strong>Planner Server</strong> computes global paths from start to goal, typically using graph-search algorithms on occupancy grid maps. The <strong>Controller Server</strong> executes local trajectory following, continuously adjusting velocity commands to track the global path while avoiding dynamic obstacles. The <strong>Recoveries Server</strong> implements fallback behaviors when navigation gets stuck—backing up, spinning in place, clearing sensor data. The <strong>Waypoint Follower</strong> handles multi-point navigation missions, sequencing through goal poses. Each server runs as a lifecycle node, allowing coordinated startup, shutdown, and reconfiguration—a major reliability improvement over Nav1&#x27;s unstructured node management.</p>
<p><strong>Costmap configuration</strong> provides the spatial representation Nav2 uses for planning and control. Each navigation server maintains costmaps—2D grid maps where each cell encodes traversability cost (0 = free space, 255 = lethal obstacle). The <strong>global costmap</strong> covers a large area (tens of meters) for long-horizon planning, typically built from a pre-mapped environment. The <strong>local costmap</strong> covers a smaller region (few meters) centered on the robot, updated frequently from real-time sensor data to detect dynamic obstacles. Costmaps layer multiple data sources: the <strong>static layer</strong> from pre-built maps, the <strong>obstacle layer</strong> from depth cameras or lidar, the <strong>inflation layer</strong> that expands obstacles to account for robot footprint, and the <strong>voxel layer</strong> that uses 3D occupancy for multi-floor reasoning. Tuning costmap parameters—update frequency, size, resolution, inflation radius—directly impacts navigation performance and safety.</p>
<p><strong>Humanoid-specific navigation challenges</strong> require careful configuration beyond differential-drive robots. Bipedal locomotion introduces instability: aggressive turns or sudden stops can cause the robot to fall, so velocity limits must be conservative (0.3-0.8 m/s typical walking speeds, limited angular velocities). The <strong>footprint</strong> parameter defines the robot&#x27;s 2D shape for collision checking—humanoids have irregular footprints (shoulders wider than hips, arms extending laterally) requiring polygon definitions rather than simple circles. <strong>Narrow corridors and doorways</strong> challenge humanoids more than wheeled robots: a 0.9m wide doorway leaves minimal clearance, demanding precise localization and control. <strong>Dynamic balance constraints</strong> mean acceleration limits must prevent destabilizing inertial forces—Nav2&#x27;s controller plugins need tuning to respect these physical limits, often requiring custom controllers that interface with the humanoid&#x27;s balance controller (Chapter 11). Unlike wheeled robots that can stop instantly, humanoids need controlled deceleration to maintain stability, requiring lookahead planning and smooth velocity profiles.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="82-slam-and-localization">8.2 SLAM and Localization<a href="#82-slam-and-localization" class="hash-link" aria-label="Direct link to 8.2 SLAM and Localization" title="Direct link to 8.2 SLAM and Localization" translate="no">​</a></h2>
<p>Autonomous navigation depends fundamentally on knowing where the robot is—both building spatial maps of the environment and localizing within those maps. SLAM (Simultaneous Localization and Mapping) solves the chicken-and-egg problem: you need a map to localize, but you need to know your position to build a map. Modern SLAM algorithms solve both simultaneously, enabling robots to navigate unknown environments without pre-existing maps.</p>
<p><strong>SLAM algorithms in ROS 2</strong> come primarily in two production-ready implementations: <strong>Google Cartographer</strong> and <strong>SLAM Toolbox</strong>. Cartographer excels at handling loop closures and large-scale mapping through pose graph optimization—as the robot revisits previously mapped areas, Cartographer detects the loop closure and adjusts the entire map to minimize accumulated drift. This makes it ideal for large buildings or outdoor environments where drift accumulates over long traversals. Cartographer supports both 2D and 3D SLAM, using lidar or depth cameras as input. The algorithm maintains submap structures that get continually optimized as new sensor data arrives, producing globally consistent maps even after kilometers of travel. SLAM Toolbox, specifically designed for Nav2 integration, provides similar capabilities with an emphasis on usability and ROS 2 native design. It offers asynchronous (online) and synchronous mapping modes, serialization for saving/loading maps mid-session, and localization mode that lets you switch from mapping to localization once a map is complete. For humanoid robots navigating homes or offices, SLAM Toolbox&#x27;s tight Nav2 integration and user-friendly tools make it the practical choice.</p>
<p><strong>Map representations</strong> determine how spatial information gets encoded for navigation. <strong>Occupancy grid maps</strong> represent the environment as a 2D grid where each cell holds a probability: 0 = definitely free, 100 = definitely occupied, -1 = unknown. These grids get generated from range sensor data (lidar, depth cameras) by projecting measurements into the map frame and updating cell probabilities using inverse sensor models. Occupancy grids work well for static structured environments—walls, furniture, corridors—and integrate naturally with Nav2&#x27;s costmap system. Resolution matters: 5cm grids capture fine detail but consume memory and computation; 10-20cm grids balance detail with efficiency for typical indoor navigation. <strong>Semantic maps</strong> extend occupancy grids with semantic labels: this region is &quot;kitchen,&quot; that obstacle is &quot;chair,&quot; this area is &quot;hallway.&quot; Semantic information enables higher-level reasoning—&quot;navigate to the kitchen&quot; rather than &quot;navigate to coordinates (5.3, -2.1)&quot;—and supports task planning. Semantic SLAM combines traditional SLAM with object detection and scene segmentation (Chapter 7), associating detected objects and regions with map locations. For humanoids performing tasks like &quot;bring me a cup from the kitchen,&quot; semantic maps bridge navigation and manipulation planning.</p>
<p><strong>AMCL (Adaptive Monte Carlo Localization)</strong> handles localization once a map exists—determining the robot&#x27;s pose within a known map using real-time sensor data. AMCL represents the robot&#x27;s belief about its position as a particle cloud: each particle is a hypothesis (x, y, θ), and particle weights reflect how well sensor observations match the map at that pose. As the robot moves, particles get resampled—likely poses receive more particles, unlikely ones get pruned—converging toward the true position. AMCL requires initial pose estimates (typically provided via RViz &quot;2D Pose Estimate&quot; tool or programmatically) and performs best with distinctive map features. In feature-poor environments (long corridors with uniform walls), AMCL may struggle with ambiguity—multiple poses fit the observations equally well. For humanoids, AMCL&#x27;s main challenge is handling sensor height: most SLAM/localization assumes sensors near ground level, but humanoid head-mounted cameras sit 1.5-1.8m high, changing what they observe. Configuring AMCL with appropriate sensor models (beam vs. likelihood field) and tuning particle count (more particles = better accuracy but higher computation) ensures reliable localization.</p>
<p><strong>Map saving and loading with map_server</strong> enables persistent maps that robots can reuse across sessions. After creating a map via SLAM, the <code>map_saver</code> node serializes the occupancy grid to a YAML metadata file (resolution, origin, thresholds) and a PGM image file (grayscale pixels encoding occupancy). Launch files for Nav2 specify these map files, and <code>map_server</code> loads them at startup, publishing the static map for AMCL and the global costmap. This workflow—map once, localize many times—suits deployed robots operating in fixed environments. For environments that change (furniture rearranged, doors opened/closed), Nav2 supports map updates: the local costmap&#x27;s obstacle layer detects discrepancies between the static map and current sensor data, adding temporary obstacles to avoid collisions while the global map remains unchanged.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="83-path-planning-and-control">8.3 Path Planning and Control<a href="#83-path-planning-and-control" class="hash-link" aria-label="Direct link to 8.3 Path Planning and Control" title="Direct link to 8.3 Path Planning and Control" translate="no">​</a></h2>
<p>Path planning transforms the high-level goal (&quot;navigate to the kitchen&quot;) into executable motion—computing collision-free paths through the environment and generating velocity commands that track those paths while respecting robot dynamics. Nav2 separates this into global planning (long-horizon paths) and local control (short-horizon reactive execution), each using pluggable algorithms optimized for different aspects of the navigation problem.</p>
<p><strong>Global planners</strong> compute paths from the robot&#x27;s current position to the goal across the entire map, treating navigation as a graph search problem on the costmap grid. <strong>NavFn</strong> implements Dijkstra&#x27;s algorithm with cost propagation—starting from the goal, it expands outward computing the minimum cost to reach each cell, then traces the gradient from start to goal to extract the path. NavFn guarantees finding the shortest path if one exists and handles arbitrary costmap configurations, but produces geometric paths without considering robot dynamics (sharp corners, unrealistic turns). <strong>Smac Planner</strong> (State Lattice with Analytic Expansions) improves on this by searching over feasible robot states—(x, y, θ) poses—rather than just positions, producing kinematically feasible paths that respect turning radius constraints. Smac Planner supports multiple motion models: Ackermann (car-like steering), differential drive, and omnidirectional. For humanoids, the differential drive model approximates bipedal locomotion—the robot can turn in place and move forward/backward—though custom motion primitives could enforce walking-specific constraints. Smac Planner uses hybrid A* search with analytic expansion for efficiency, computing paths in tens of milliseconds even in large maps.</p>
<p><strong>Local controllers</strong> execute the global plan while handling dynamic obstacles and tracking errors. Unlike global planners that assume a static costmap, controllers run at high frequency (10-30 Hz) and react to real-time sensor data. <strong>DWB (Dynamic Window Approach)</strong> generates candidate velocity trajectories by sampling (v_x, v_y, ω) tuples within dynamic constraints—the &quot;dynamic window&quot; of achievable velocities given current velocity and acceleration limits. Each candidate trajectory gets forward-simulated over 1-3 seconds, scored based on multiple criteria (goal alignment, path following, obstacle clearance, smoothness), and the highest-scoring trajectory&#x27;s initial velocity gets commanded to the robot. DWB&#x27;s sampling-based approach makes it flexible and easy to tune, but it can struggle in tight spaces where few samples are collision-free. <strong>TEB (Timed Elastic Band)</strong> formulates control as an optimization problem: it represents the trajectory as a sequence of timed poses connected by &quot;elastic bands&quot; and optimizes this trajectory to minimize travel time, deviation from the global plan, and proximity to obstacles, while satisfying velocity and acceleration constraints. TEB produces smoother, more efficient paths than DWB and handles narrow passages better, but requires more computation and careful parameter tuning. <strong>MPPI (Model Predictive Path Integral)</strong> applies model-predictive control with sampling—it forward-simulates thousands of noisy trajectories using a dynamics model, evaluates their cost, and computes the optimal control as a probability-weighted average. MPPI excels at aggressive dynamic maneuvers and can incorporate complex cost functions, making it attractive for high-performance navigation.</p>
<p><strong>Footprint configuration</strong> defines the robot&#x27;s 2D shape for collision checking, critical for humanoids with non-circular profiles. Nav2 supports multiple footprint specifications: circular (single radius parameter), square/rectangular (width and height), or polygon (arbitrary vertex list). Humanoids require polygon footprints capturing body shape—shoulders, hips, and extended arms if relevant to the task. The footprint gets projected along planned trajectories to verify collision-free motion. Conservative footprints improve safety but restrict navigation—a footprint including fully extended arms prevents passing through doorways even though arms can be tucked. Some implementations use task-dependent footprints: compact for navigation through tight spaces, extended when carrying objects. The footprint parameter appears in both global planner (for path computation) and local controller (for trajectory evaluation) configurations, and must match the robot&#x27;s true dimensions plus safety margins.</p>
<p><strong>Velocity and acceleration limits</strong> for bipedal stability prevent the navigation system from commanding motions that destabilize the robot. Maximum velocities (v_x, v_y, ω) must stay well below the robot&#x27;s dynamic limits—typically 0.3-0.8 m/s linear, 0.5-1.0 rad/s angular for walking humanoids. Acceleration limits prevent abrupt changes: humanoid balance controllers need time to compensate for inertial forces, so commanded velocity changes should ramp over 0.5-1.0 seconds. Nav2 controllers accept these limits as parameters: <code>max_vel_x</code>, <code>max_vel_theta</code>, <code>acc_lim_x</code>, <code>acc_lim_theta</code>. Tuning involves testing: command progressively higher velocities and accelerations until instability appears, then back off to safe values. For advanced integration, the controller can query the humanoid&#x27;s balance state via topics or services—if the robot detects instability, it requests the controller to reduce speed or pause navigation until balance is restored.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="84-behavior-trees-and-recovery">8.4 Behavior Trees and Recovery<a href="#84-behavior-trees-and-recovery" class="hash-link" aria-label="Direct link to 8.4 Behavior Trees and Recovery" title="Direct link to 8.4 Behavior Trees and Recovery" translate="no">​</a></h2>
<p>Navigation failures are inevitable—sensors get occluded, dynamic obstacles block paths, localization drifts, or the robot gets physically stuck. Robust navigation systems must detect failures and attempt recovery rather than freezing or requiring human intervention. Nav2 uses behavior trees to implement flexible, hierarchical navigation logic that handles these scenarios gracefully.</p>
<p><strong>Behavior tree fundamentals</strong> structure decision logic as a tree of nodes evaluated each control cycle. Internal nodes control execution flow: <strong>Sequence</strong> nodes execute children left-to-right until one fails (logical AND), <strong>Fallback</strong> nodes execute children until one succeeds (logical OR), <strong>Parallel</strong> nodes execute multiple children simultaneously. Leaf nodes are <strong>Action</strong> nodes (execute tasks like &quot;compute plan&quot; or &quot;follow path&quot;) or <strong>Condition</strong> nodes (check predicates like &quot;is path valid?&quot;). Each node returns SUCCESS, FAILURE, or RUNNING (still executing). This structure enables readable, modular navigation logic: &quot;Try to follow the path; if that fails, try recovery behavior A; if that fails, try recovery behavior B; if all fail, abort.&quot; Behavior trees offer key advantages over state machines: they compose hierarchically (subtrees can be reused), add new behaviors without modifying existing logic, and explicitly represent concurrent tasks (e.g., path following while monitoring battery).</p>
<p><strong>Nav2&#x27;s default behavior tree</strong> implements a standard navigate-to-goal workflow. The root Fallback node attempts the primary navigation sequence: compute initial plan, enter control loop (follow path while replanning periodically), succeed when goal reached. If the primary sequence fails—planning fails, path following stalls, timeout exceeded—the Fallback node executes recovery behaviors in sequence. The default recoveries: <strong>clear costmap</strong> (remove spurious obstacles from sensor noise), <strong>spin</strong> (rotate 360° to relocalize and detect previously unseen obstacles), <strong>backup</strong> (reverse away from close obstacles), <strong>wait</strong> (pause briefly in case a dynamic obstacle clears). If all recoveries fail, the behavior tree returns FAILURE, triggering higher-level task logic to handle the navigation failure (request human help, try alternate goal, abort task). This structure makes navigation robust to common failures without requiring manual failure handling in application code.</p>
<p><strong>Custom behaviors for humanoid-specific actions</strong> extend Nav2&#x27;s capabilities beyond generic mobile base navigation. Humanoids might need behaviors like <strong>approach human</strong> (navigate close to a person while maintaining social distance and orienting to face them), <strong>traverse doorway</strong> (precisely navigate through narrow doors, possibly with arm tucking or sideways turning), <strong>sit/stand</strong> (transition between mobility and seated postures), or <strong>check balance</strong> (pause navigation if bipedal instability detected, resume when stable). Implementing custom behaviors involves creating behavior tree action plugins: C++ classes inheriting from <code>nav2_behavior_tree::BtActionNode</code>, implementing <code>on_tick()</code> (called each behavior tree cycle), <code>on_success()/on_failure()</code> (called when action completes), and registering the plugin with Nav2. The custom behaviors can access Nav2 APIs (costmaps, TF, planners) and robot-specific interfaces (balance controller, manipulation state), enabling tight integration between navigation and other robot systems.</p>
<p><strong>Recovery behaviors</strong> serve as fallback strategies when primary navigation fails, attempting to resolve common failure modes before giving up. <strong>Clear costmap recovery</strong> resets transient obstacles that might be sensor noise or stale data—useful when the robot perceives phantom obstacles blocking its path. <strong>Spin recovery</strong> rotates in place, helping relocalization recover from position uncertainty and revealing obstacles behind the robot. <strong>Backup recovery</strong> moves backward a short distance, useful when the robot gets too close to obstacles for the controller to maneuver. <strong>Wait recovery</strong> pauses briefly, effective for temporary dynamic obstacles (people walking past, doors opening). Recovery behaviors have limited scope—they handle transient issues but can&#x27;t resolve fundamental problems like blocked goals or broken sensors. Nav2 limits recovery attempts (maximum count, timeout) to prevent infinite recovery loops when a situation truly is unrecoverable. For humanoids, additional recoveries might include <strong>rebalance</strong> (explicitly stabilize bipedal stance), <strong>narrow mode</strong> (reduce footprint and enable cautious maneuvering), or <strong>request human assistance</strong> (vocalize need for help and wait for intervention).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercises">Exercises<a href="#exercises" class="hash-link" aria-label="Direct link to Exercises" title="Direct link to Exercises" translate="no">​</a></h2>
<ol>
<li class=""><strong>SLAM Mapping</strong>: Create a map of a simulated indoor environment using SLAM Toolbox</li>
<li class=""><strong>Localization</strong>: Implement AMCL on a pre-built map and verify pose accuracy</li>
<li class=""><strong>Waypoint Navigation</strong>: Navigate between multiple waypoints autonomously</li>
<li class=""><strong>Obstacle Avoidance</strong>: Test dynamic obstacle avoidance with moving objects</li>
<li class=""><strong>Custom Behavior</strong>: Design a behavior tree that approaches a person when called</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<ul>
<li class="">Nav2 provides production-ready autonomous navigation for ROS 2 robots</li>
<li class="">SLAM enables robots to build maps and localize simultaneously</li>
<li class="">Behavior trees offer flexible, modular navigation logic</li>
<li class="">Humanoid navigation requires careful tuning for bipedal stability</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class="">Nav2 documentation and tutorials</li>
<li class="">Behavior tree fundamentals (BehaviorTree.CPP)</li>
<li class="">SLAM algorithms comparison (Cartographer vs. SLAM Toolbox)</li>
<li class="">Motion planning for bipedal robots</li>
</ul>
<hr>
<p><strong>Status</strong>: Draft complete (1,910 words)</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book/tree/main/book/docs/08-navigation.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Book/docs/07-isaac-ros"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">7. Isaac ROS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">9. VLA Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#81-nav2-architecture" class="table-of-contents__link toc-highlight">8.1 Nav2 Architecture</a></li><li><a href="#82-slam-and-localization" class="table-of-contents__link toc-highlight">8.2 SLAM and Localization</a></li><li><a href="#83-path-planning-and-control" class="table-of-contents__link toc-highlight">8.3 Path Planning and Control</a></li><li><a href="#84-behavior-trees-and-recovery" class="table-of-contents__link toc-highlight">8.4 Behavior Trees and Recovery</a></li><li><a href="#exercises" class="table-of-contents__link toc-highlight">Exercises</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone">Capstone Project</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.omniverse.nvidia.com/isaacsim/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI Robotics Team. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>