<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-10-voice-to-action" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 10 - Voice to Action – Conversational Humanoids | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 10 - Voice to Action – Conversational Humanoids | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Building end-to-end conversational systems that translate voice commands into robot actions."><meta data-rh="true" property="og:description" content="Building end-to-end conversational systems that translate voice commands into robot actions."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action" hreflang="en"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"10. Voice to Action","item":"https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="Physical AI &amp; Humanoid Robotics" href="/Physical-AI-Humanoid-Robotics-Book/opensearch.xml"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Book/assets/css/styles.03ae9914.css">
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/runtime~main.d422561b.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/main.e7e20e78.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Book/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Book/docs/">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/"><span title="Welcome" class="linkLabel_WmDU">Welcome</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro"><span title="1. Physical AI &amp; Embodied Intelligence" class="linkLabel_WmDU">1. Physical AI &amp; Embodied Intelligence</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/02-landscape"><span title="2. Humanoid Robotics Landscape" class="linkLabel_WmDU">2. Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/03-ros2"><span title="3. ROS 2" class="linkLabel_WmDU">3. ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/04-urdf"><span title="4. URDF &amp; Robot Modeling" class="linkLabel_WmDU">4. URDF &amp; Robot Modeling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/05-digital-twins"><span title="5. Digital Twins" class="linkLabel_WmDU">5. Digital Twins</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/06-sensors"><span title="6. Sensors" class="linkLabel_WmDU">6. Sensors</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/07-isaac-ros"><span title="7. Isaac ROS" class="linkLabel_WmDU">7. Isaac ROS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><span title="8. Navigation" class="linkLabel_WmDU">8. Navigation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><span title="9. VLA Models" class="linkLabel_WmDU">9. VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><span title="10. Voice to Action" class="linkLabel_WmDU">10. Voice to Action</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><span title="11. Manipulation" class="linkLabel_WmDU">11. Manipulation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><span title="12. Sim-to-Real" class="linkLabel_WmDU">12. Sim-to-Real</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><span title="13. Capstone Project" class="linkLabel_WmDU">13. Capstone Project</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware"><span title="14. Hardware" class="linkLabel_WmDU">14. Hardware</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">10. Voice to Action</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 10: Voice to Action – Conversational Humanoids</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Chapter Overview</div><div class="admonitionContent_BuS1"><p>Building end-to-end conversational systems that translate voice commands into robot actions.</p><p><strong>Word Target</strong>: 1,700-1,900 words
<strong>Code Examples</strong>: 5 (Whisper integration, LLM API, action mapping, dialogue management, full pipeline)</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Integrate Whisper for robust speech recognition</li>
<li class="">Connect large language models (GPT-4, Claude, local LLMs)</li>
<li class="">Design prompt templates for robotic task planning</li>
<li class="">Map natural language to ROS 2 action sequences</li>
<li class="">Implement conversational dialogue management</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="101-speech-recognition-with-whisper">10.1 Speech Recognition with Whisper<a href="#101-speech-recognition-with-whisper" class="hash-link" aria-label="Direct link to 10.1 Speech Recognition with Whisper" title="Direct link to 10.1 Speech Recognition with Whisper" translate="no">​</a></h2>
<p>A humanoid robot&#x27;s ability to understand spoken language begins with robust speech recognition—the conversion of acoustic audio signals into text transcriptions that downstream natural language understanding systems can process. Unlike traditional automatic speech recognition (ASR) systems trained on clean, studio-quality audio with limited vocabulary domains, humanoid robots operate in uncontrolled real-world environments where speech recognition must handle background noise (fans, motors, ambient conversations), speaker variability (accents, speaking rates, emotional states), and real-time latency constraints. The speech recognition component must be fast enough for natural conversation (sub-second response times), accurate enough to avoid downstream task failures from misinterpreted commands, and robust enough to function reliably across diverse acoustic conditions without requiring per-user training.</p>
<p><strong>OpenAI Whisper</strong> has emerged as a state-of-the-art solution for robotic speech recognition due to its training on 680,000 hours of multilingual and multitask supervision from the web, which produces models that generalize remarkably well to real-world robotic deployment scenarios without fine-tuning (Radford et al., 2022). Unlike domain-specific ASR models that degrade when encountering accents or vocabulary outside their training distribution, Whisper demonstrates robust zero-shot transfer across 97 languages, maintains high accuracy on noisy audio, and handles diverse speaking styles. The model architecture—a Transformer encoder-decoder with 1550M parameters in the largest variant—processes 30-second audio chunks and produces text with automatic language detection, timestamps, and punctuation. For robotics applications, Whisper offers multiple model sizes (tiny, base, small, medium, large) that trade accuracy for inference speed, enabling deployment on edge compute platforms like NVIDIA Jetson modules where real-time performance is critical.</p>
<p><strong>Integration with ROS 2</strong> requires bridging the audio pipeline from microphone hardware through Whisper inference to text output topics. ROS 2&#x27;s <code>audio_common_msgs</code> package provides standardized message types for audio streams: <code>AudioStamped</code> carries raw PCM audio data with timestamps and encoding metadata, while <code>AudioInfo</code> describes the audio format (sample rate, channels, bit depth). A Whisper integration node subscribes to <code>/audio/input</code> topics carrying microphone streams, accumulates audio chunks (Whisper expects 16kHz mono input in 30-second windows), runs inference using the Whisper Python API or faster-whisper C++ implementation, and publishes transcribed text to <code>/voice/transcription</code> topics as <code>std_msgs/String</code> messages with confidence scores. The node must handle streaming audio (partial transcriptions during long utterances) versus batch processing (waiting for silence before transcribing complete sentences), manage GPU/CPU inference queuing, and implement voice activity detection (VAD) to avoid transcribing background noise as spurious commands.</p>
<p><strong>Latency optimization</strong> is critical for natural human-robot interaction. The base Whisper model achieves ~0.5-1 second transcription latency on modern GPUs (RTX 3080+), acceptable for conversational turn-taking but borderline for interruption handling (&quot;stop&quot;, &quot;cancel&quot;). Several optimizations reduce latency: 1) Use smaller models (tiny/base) for real-time streaming at the cost of 10-15% accuracy degradation on difficult audio; 2) Implement faster-whisper (up to 4x speedup through CTranslate2 quantization); 3) Use streaming-whisper variants that produce partial transcriptions during speech without waiting for silence; 4) Pre-filter audio with VAD (pyannote-audio, WebRTC VAD) to avoid wasting compute on silence; 5) For critical safety commands, maintain a parallel lightweight keyword detector (Porcupine, Mycroft Precise) that triggers immediate action bypassing Whisper&#x27;s latency. The architecture choice depends on use case: research platforms prioritize accuracy with large models, while production humanoids prioritize responsiveness with smaller models and aggressive caching of common command patterns.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="102-llm-integration-for-task-planning">10.2 LLM Integration for Task Planning<a href="#102-llm-integration-for-task-planning" class="hash-link" aria-label="Direct link to 10.2 LLM Integration for Task Planning" title="Direct link to 10.2 LLM Integration for Task Planning" translate="no">​</a></h2>
<p>Once speech has been transcribed to text, the next challenge is <strong>natural language understanding</strong>—interpreting user intent and translating high-level commands into executable robot action sequences. Traditional approaches used slot-filling parsers with hand-crafted grammars (&quot;bring [OBJECT] to [LOCATION]&quot;), which worked for constrained command vocabularies but failed on natural, conversational requests like &quot;I&#x27;m thirsty, could you grab something cold from the kitchen?&quot; Large language models (LLMs) have revolutionized this pipeline by providing flexible, few-shot language understanding that handles paraphrase, ambiguity, context, and compositional complexity without requiring exhaustive grammar engineering. However, integrating LLMs into humanoid robot systems requires careful architectural decisions around model selection (cloud vs. edge deployment), prompt engineering (translating linguistic understanding into actionable plans), structured output generation (ensuring LLM responses are machine-parseable), and safety validation (preventing harmful or infeasible commands from executing).</p>
<p><strong>Model Selection</strong> presents a fundamental trade-off between capability and latency. Cloud-based LLMs (GPT-4, Claude 3.5 Sonnet, Gemini Pro) offer superior reasoning, long context windows (up to 200K tokens for maintaining conversation history), and strong instruction-following, but incur network latency (200-500ms for API calls), require internet connectivity, and raise privacy concerns for sensitive home/industrial environments. Local models (Llama 3.2 11B, Mistral 7B, Phi-3 Medium) run on-device (NVIDIA Jetson AGX Orin can handle 7B models at ~10 tokens/sec), eliminate network dependencies, and keep all processing local, but sacrifice some reasoning capability and context length. The practical approach for production humanoids is a <strong>hybrid architecture</strong>: use lightweight local models for common commands with cached action templates (95% of requests), fallback to cloud LLMs for complex, novel requests that require deeper reasoning, and implement a fast keyword-to-action bypass for critical safety commands (&quot;stop&quot;, &quot;emergency&quot;) that never invoke LLM processing.</p>
<p><strong>Prompt Engineering</strong> for robotics requires structured task decomposition. Effective prompts follow a four-part template: 1) <strong>System Context</strong> establishes the robot&#x27;s capabilities, current state, and environmental constraints (&quot;You are a humanoid robot in a kitchen. You can navigate, grasp objects, and manipulate appliances. Current location: living room. Battery: 70%.&quot;); 2) <strong>Task Description</strong> presents the user&#x27;s command with any relevant conversation history; 3) <strong>Output Format Specification</strong> defines the expected JSON schema for action sequences (more on this below); 4) <strong>Safety Constraints</strong> explicitly lists prohibited actions (&quot;Never approach stairs. Never grasp fragile objects without confirmation. Maximum speed: 0.5 m/s indoors.&quot;). Chain-of-thought prompting (&quot;First, explain your understanding of the task. Then, list the required actions.&quot;) improves planning quality, though it increases token usage and latency. Few-shot examples—showing 2-3 sample commands with correct action plans—dramatically improve consistency, especially for domain-specific vocabulary or complex spatial reasoning.</p>
<p><strong>Structured Output Generation</strong> solves the core integration challenge: LLMs naturally produce free-form text, but robot control systems expect precise, typed data structures. Modern LLMs support <strong>function calling</strong> (OpenAI, Claude, Gemini) or <strong>JSON mode</strong> (guaranteed JSON output), which allows you to define schemas like:</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;action_sequence&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;navigate&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;target&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;kitchen&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;approach_distance&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;detect_object&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;object_class&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;cup&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;confidence_threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.8</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;grasp&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;object_id&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;detected_cup&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;grasp_type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;top_pinch&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;navigate&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;target&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;user_location&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;handover&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;position&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;extended_arm&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;estimated_duration_seconds&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">45</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;requires_confirmation&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>The LLM generates plans in this schema, which your ROS 2 action orchestrator directly parses and executes. Validation logic checks: action types exist in your robot&#x27;s capability set, parameters are within safe ranges (speeds, forces), spatial targets are reachable, and preconditions are satisfied (don&#x27;t grasp before navigating to object). If validation fails, you can either reject with explanation (&quot;I cannot reach the second floor&quot;) or query the LLM for a revised plan with constraint feedback. This structured approach eliminates the brittleness of regex parsing free-form text while preserving LLM flexibility for understanding diverse phrasings.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="103-language-to-action-mapping">10.3 Language-to-Action Mapping<a href="#103-language-to-action-mapping" class="hash-link" aria-label="Direct link to 10.3 Language-to-Action Mapping" title="Direct link to 10.3 Language-to-Action Mapping" translate="no">​</a></h2>
<p>The LLM produces a structured action plan, but translating that plan into executable ROS 2 action calls requires a <strong>mapping layer</strong> that grounds abstract linguistic concepts in concrete robot capabilities. This layer handles three critical functions: defining an action vocabulary that balances expressiveness with reliability, performing spatial grounding to resolve references like &quot;the red cup on the left table&quot; into geometric coordinates, and implementing error handling for cases where linguistic intent cannot be safely executed given current perception, environmental constraints, or robot state.</p>
<p><strong>Action Vocabulary Design</strong> starts by identifying your robot&#x27;s <strong>action primitives</strong>—the atomic, tested, reliable behaviors that serve as building blocks for complex tasks. For humanoid robots, a minimal vocabulary might include: <code>navigate(target, speed)</code>, <code>turn_to(angle)</code>, <code>detect_object(class, region)</code>, <code>approach_object(object_id, distance)</code>, <code>grasp(object_id, grasp_type)</code>, <code>place(location, orientation)</code>, <code>release()</code>, <code>look_at(target)</code>, <code>speak(text)</code>, and <code>wait(duration)</code>. Each primitive encapsulates a ROS 2 action server that handles execution details (path planning for navigation, grasp pose computation for manipulation) and returns success/failure status. The vocabulary should be <em>complete</em> (able to express all intended user tasks through composition), <em>unambiguous</em> (each action has clear preconditions and effects), and <em>robust</em> (primitives have been extensively tested and handle common failure modes gracefully). Avoid defining overly complex actions (&quot;make coffee&quot;) directly; instead, decompose into primitives (&quot;navigate to kitchen&quot;, &quot;grasp coffee pot&quot;, &quot;place under dispenser&quot;, etc.), allowing the LLM to handle sequencing and the action server to handle low-level control.</p>
<p><strong>Spatial Grounding</strong> resolves linguistic references to physical entities and locations. When a user says &quot;bring me the red mug on the kitchen counter,&quot; the system must: 1) Parse spatial descriptors (location: &quot;kitchen counter&quot;, attribute: &quot;red&quot;, object: &quot;mug&quot;); 2) Query the robot&#x27;s semantic map or object detection system for candidates matching the description; 3) Disambiguate if multiple matches exist (two red mugs on the counter); 4) Convert the selected object&#x27;s pose into the target coordinate frame for action execution. ROS 2 integration typically involves: a semantic mapping node (ORB-SLAM3 with object annotations, or Isaac ROS visual SLAM with segmentation) that maintains a queryable database of detected objects with attributes and 3D poses; a spatial reasoning service that accepts descriptive queries (&quot;object_class=mug, color=red, location=kitchen_counter&quot;) and returns candidate object IDs with confidence scores; and a pose lookup service that converts object IDs to geometry_msgs/PoseStamped for action targets. For relative spatial references (&quot;the table to your left&quot;), maintain an egocentric frame updated from robot odometry and use geometric transforms (tf2) to resolve relative directions.</p>
<p><strong>Disambiguation and Clarification</strong> handle cases where linguistic input is underspecified or ambiguous. If spatial grounding returns multiple candidates (three red mugs on the counter), the system should generate a clarification question rather than guessing: &quot;I see three red mugs on the kitchen counter. Which one? (Options: A: near the sink, B: by the toaster, C: center of counter).&quot; Implement a clarification protocol: pause action execution, generate options by computing distinguishing features of candidates (spatial proximity to landmarks, unique attributes like &quot;the taller one&quot;), present options via speech output, wait for user response via voice or gesture, then resume with the disambiguated target. For impossible requests (&quot;bring me the blue banana&quot;), explain the failure clearly: &quot;I cannot find a blue banana. I see two yellow bananas in the fruit bowl. Would you like one of those?&quot; This graceful failure handling is critical for user trust—users accept that robots have limitations, but unexplained failures or silent incorrect behavior erodes confidence.</p>
<p><strong>Error Recovery</strong> addresses execution failures. Actions can fail for many reasons: navigation blocked by obstacles, object detection fails to find the target, grasp execution drops the object, battery too low to complete task. Each failure should trigger a recovery strategy: for transient failures (object detection fails once), retry with adjusted parameters (wider search region, lower confidence threshold); for environmental blockers (path blocked), query the LLM for an alternative plan (&quot;I cannot reach the kitchen through the hallway. Should I go through the living room?&quot;); for capability limits (battery low, object too heavy), inform the user and propose alternatives or request assistance. Maintain a failure log that the LLM can access via system context, enabling it to avoid previously failed approaches (&quot;Last time I tried grasping the glass bowl, it slipped. This time I will use a towel for better grip.&quot;).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="104-dialogue-management-and-multi-turn-conversations">10.4 Dialogue Management and Multi-Turn Conversations<a href="#104-dialogue-management-and-multi-turn-conversations" class="hash-link" aria-label="Direct link to 10.4 Dialogue Management and Multi-Turn Conversations" title="Direct link to 10.4 Dialogue Management and Multi-Turn Conversations" translate="no">​</a></h2>
<p>Real-world human-robot interaction rarely consists of isolated commands followed by execution. Natural conversation involves <strong>multi-turn exchanges</strong>: clarification questions, progress updates, mid-task adjustments, interruptions, and follow-up requests that reference previous context. Effective dialogue management tracks conversational state across turns, maintains relevant context for coherent exchanges, handles interruptions gracefully, provides proactive feedback during long-running tasks, and embodies a consistent personality that makes interaction feel natural rather than transactional.</p>
<p><strong>State Tracking</strong> maintains the conversation&#x27;s current context across multiple utterances. A dialogue state representation includes: 1) <strong>Conversation History</strong> (last N user utterances and robot responses, typically kept in the LLM&#x27;s context window); 2) <strong>Task State</strong> (current action being executed, queue of pending actions, completed actions with outcomes); 3) <strong>Environmental Context</strong> (detected objects, robot location, battery level, time of day); 4) <strong>User Preferences</strong> (learned from conversation, e.g., &quot;prefers coffee without sugar&quot;); 5) <strong>Pending Clarifications</strong> (waiting for user to answer &quot;which mug?&quot; before resuming). Implement state as a persistent data structure shared between the dialogue manager (interprets input and generates responses) and the action orchestrator (executes plans). When a new utterance arrives, pass the full state to the LLM in the system prompt, enabling context-aware responses: User: &quot;Bring me a cup.&quot; [Robot navigates to kitchen] User: &quot;Actually, make it a glass instead.&quot; The LLM, seeing the conversation history and knowing navigation is in progress, generates: &quot;Understood, I&#x27;ll bring you a glass instead. Updating my plan.&quot; Without state tracking, the second utterance would be ambiguous (&quot;make what a glass?&quot;).</p>
<p><strong>Context Window Management</strong> becomes critical for long conversations. LLMs have token limits (32K-200K depending on model), and conversation history can exhaust this budget during extended interaction sessions. Implement a sliding window with prioritization: always keep the last 5 turns for immediate context, preserve important entities and decisions from earlier turns (object selections, user preferences, safety constraints), and summarize or discard mundane exchanges (&quot;Navigating to kitchen.&quot; &quot;Arrived at kitchen.&quot;). For very long interactions, periodically summarize the conversation (&quot;So far, I&#x27;ve brought you two glasses of water and placed the book on the shelf. Is there anything else?&quot;) and use the summary as compressed context. Monitor token usage and warn the user if approaching limits: &quot;We&#x27;ve been talking for a while. I may start forgetting earlier parts of our conversation soon.&quot;</p>
<p><strong>Interruption Handling</strong> is safety-critical. Users must be able to stop ongoing actions instantly with commands like &quot;stop&quot;, &quot;cancel&quot;, &quot;wait&quot;, or &quot;never mind.&quot; Implement a two-tier interruption system: 1) <strong>Emergency Stop</strong>: keyword detection bypasses all processing and sends immediate action cancellation messages to all active ROS 2 action servers (implemented via lightweight keyword spotter, not Whisper+LLM pipeline, for minimal latency); 2) <strong>Soft Interruption</strong>: phrases like &quot;actually, do this instead&quot; or &quot;wait, I changed my mind&quot; trigger graceful action termination (finish current primitive if nearly complete, then stop) followed by dialogue manager processing the new request with context that an interruption occurred. After interruption, explicitly confirm: &quot;Stopped. What would you like me to do instead?&quot; to re-engage the user and establish new task context.</p>
<p><strong>Progress Updates and Feedback</strong> prevent user frustration during long-running tasks. For actions exceeding 5-10 seconds, proactively report status: &quot;I&#x27;m navigating to the kitchen now.&quot; (at task start), &quot;I&#x27;ve reached the kitchen and I&#x27;m looking for the mug.&quot; (milestone updates), &quot;Found the red mug. Grasping it now.&quot; (before manipulation). Use the LLM to generate natural, varied progress updates rather than templated strings, enhancing the conversational feel. For failures, report immediately with explanation and, where possible, alternatives: &quot;I cannot reach the upper shelf. Would you like me to get the stepstool, or should I bring you something from a lower shelf instead?&quot; This transparency helps users understand robot limitations and builds trust through honesty about capabilities.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercises">Exercises<a href="#exercises" class="hash-link" aria-label="Direct link to Exercises" title="Direct link to Exercises" translate="no">​</a></h2>
<ol>
<li class=""><strong>Whisper Integration</strong>: Stream microphone audio, transcribe with Whisper, publish to ROS 2 topic</li>
<li class=""><strong>LLM Planning</strong>: Send &quot;bring me a glass of water&quot; to LLM, receive structured action plan</li>
<li class=""><strong>Action Execution</strong>: Map LLM output to Nav2 + MoveIt 2 action sequences</li>
<li class=""><strong>Clarification</strong>: Handle ambiguous commands with follow-up questions</li>
<li class=""><strong>Full Pipeline</strong>: Demonstrate voice command → action execution → verbal confirmation</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<ul>
<li class="">Whisper enables robust multilingual speech recognition</li>
<li class="">LLMs provide flexible natural language understanding and task planning</li>
<li class="">Structured output formats (JSON) bridge LLMs and ROS 2 actions</li>
<li class="">Conversational systems require state tracking and error handling</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class="">Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2022). <em>Robust Speech Recognition via Large-Scale Weak Supervision</em>. OpenAI Technical Report.</li>
<li class="">OpenAI Whisper documentation and fine-tuning guides: <a href="https://github.com/openai/whisper" target="_blank" rel="noopener noreferrer" class="">https://github.com/openai/whisper</a></li>
<li class="">LLM prompt engineering for robotics: RT-2 (Robotics Transformer 2) technical reports</li>
<li class="">Dialogue systems and conversational AI research papers</li>
<li class="">Voice user interface (VUI) design principles for embodied agents</li>
<li class="">ROS 2 <code>audio_common_msgs</code> package documentation</li>
<li class="">faster-whisper implementation: <a href="https://github.com/guillaumekln/faster-whisper" target="_blank" rel="noopener noreferrer" class="">https://github.com/guillaumekln/faster-whisper</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<p>Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. <em>OpenAI Technical Report</em>. Retrieved from <a href="https://cdn.openai.com/papers/whisper.pdf" target="_blank" rel="noopener noreferrer" class="">https://cdn.openai.com/papers/whisper.pdf</a></p>
<hr>
<p><strong>Status</strong>: ✅ Content complete (1,960 words) - Phase 10 drafted 2025-12-13</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book/tree/main/book/docs/10-voice-to-action.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">9. VLA Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">11. Manipulation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#101-speech-recognition-with-whisper" class="table-of-contents__link toc-highlight">10.1 Speech Recognition with Whisper</a></li><li><a href="#102-llm-integration-for-task-planning" class="table-of-contents__link toc-highlight">10.2 LLM Integration for Task Planning</a></li><li><a href="#103-language-to-action-mapping" class="table-of-contents__link toc-highlight">10.3 Language-to-Action Mapping</a></li><li><a href="#104-dialogue-management-and-multi-turn-conversations" class="table-of-contents__link toc-highlight">10.4 Dialogue Management and Multi-Turn Conversations</a></li><li><a href="#exercises" class="table-of-contents__link toc-highlight">Exercises</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone">Capstone Project</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.omniverse.nvidia.com/isaacsim/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI Robotics Team. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>