<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-11-manipulation" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 11 - Object Manipulation with MoveIt 2 | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 11 - Object Manipulation with MoveIt 2 | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Complete guide to robotic manipulation using MoveIt 2 for humanoid arms and hands."><meta data-rh="true" property="og:description" content="Complete guide to robotic manipulation using MoveIt 2 for humanoid arms and hands."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation" hreflang="en"><link data-rh="true" rel="alternate" href="https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"11. Manipulation","item":"https://humaafatima.github.io/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="Physical AI &amp; Humanoid Robotics" href="/Physical-AI-Humanoid-Robotics-Book/opensearch.xml"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Book/assets/css/styles.03ae9914.css">
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/runtime~main.d422561b.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Book/assets/js/main.e7e20e78.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Book/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Book/docs/">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/"><span title="Welcome" class="linkLabel_WmDU">Welcome</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro"><span title="1. Physical AI &amp; Embodied Intelligence" class="linkLabel_WmDU">1. Physical AI &amp; Embodied Intelligence</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/02-landscape"><span title="2. Humanoid Robotics Landscape" class="linkLabel_WmDU">2. Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/03-ros2"><span title="3. ROS 2" class="linkLabel_WmDU">3. ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/04-urdf"><span title="4. URDF &amp; Robot Modeling" class="linkLabel_WmDU">4. URDF &amp; Robot Modeling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/05-digital-twins"><span title="5. Digital Twins" class="linkLabel_WmDU">5. Digital Twins</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/06-sensors"><span title="6. Sensors" class="linkLabel_WmDU">6. Sensors</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/07-isaac-ros"><span title="7. Isaac ROS" class="linkLabel_WmDU">7. Isaac ROS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/08-navigation"><span title="8. Navigation" class="linkLabel_WmDU">8. Navigation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/09-vla-models"><span title="9. VLA Models" class="linkLabel_WmDU">9. VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><span title="10. Voice to Action" class="linkLabel_WmDU">10. Voice to Action</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Physical-AI-Humanoid-Robotics-Book/docs/11-manipulation"><span title="11. Manipulation" class="linkLabel_WmDU">11. Manipulation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><span title="12. Sim-to-Real" class="linkLabel_WmDU">12. Sim-to-Real</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone"><span title="13. Capstone Project" class="linkLabel_WmDU">13. Capstone Project</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/14-hardware"><span title="14. Hardware" class="linkLabel_WmDU">14. Hardware</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-Book/docs/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">11. Manipulation</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 11: Object Manipulation with MoveIt 2</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Chapter Overview</div><div class="admonitionContent_BuS1"><p>Complete guide to robotic manipulation using MoveIt 2 for humanoid arms and hands.</p><p><strong>Word Target</strong>: 1,700-1,900 words
<strong>Code Examples</strong>: 5 (MoveIt setup, motion planning, grasp planning, pick-place, dual-arm coordination)</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Configure MoveIt 2 for humanoid arm kinematics</li>
<li class="">Plan collision-free motion trajectories</li>
<li class="">Implement grasp planning and execution</li>
<li class="">Perform pick-and-place manipulation tasks</li>
<li class="">Coordinate dual-arm manipulation</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="111-moveit-2-architecture">11.1 MoveIt 2 Architecture<a href="#111-moveit-2-architecture" class="hash-link" aria-label="Direct link to 11.1 MoveIt 2 Architecture" title="Direct link to 11.1 MoveIt 2 Architecture" translate="no">​</a></h2>
<p>Humanoid manipulation—grasping objects, placing them precisely, coordinating dual arms for bimanual tasks—requires sophisticated motion planning that handles high-dimensional configuration spaces (7+ DOF arms), real-time collision avoidance with environment and self-collision constraints, and tight integration between perception (where is the object?), planning (how do I reach it?), and control (execute the motion smoothly). <strong>MoveIt 2</strong> is the de facto standard motion planning framework for ROS 2, providing a complete pipeline from high-level task specification (&quot;grasp the cup&quot;) to low-level joint trajectory execution, with production-tested reliability across thousands of deployed robots in research, industry, and service applications.</p>
<p><strong>Core Architecture</strong> follows a modular design with three primary subsystems: <strong>Planning</strong>, <strong>Execution</strong>, and <strong>Perception Integration</strong>. The planning subsystem interfaces with multiple motion planning libraries (primarily OMPL, the Open Motion Planning Library) to compute collision-free trajectories between configurations. You specify a planning group (e.g., &quot;right_arm&quot; with shoulder, elbow, wrist joints), a start state (current joint positions), and a goal specification (target end-effector pose or joint configuration), and MoveIt invokes a planning algorithm (RRT, RRT*, PRM, STOMP) to find a feasible path through the robot&#x27;s configuration space. The execution subsystem manages the interface to hardware or simulation, sending computed trajectories to ROS 2 controllers (typically <code>JointTrajectoryController</code> for position control or <code>JointTrajectoryController</code> with impedance for compliant manipulation) and monitoring execution feedback. The perception integration subsystem processes sensor data (point clouds from depth cameras, occupancy maps from 3D sensors) to dynamically update the planning scene&#x27;s collision environment, enabling robots to react to moving obstacles or updated object locations.</p>
<p><strong>OMPL Integration</strong> provides MoveIt with a library of sampling-based motion planning algorithms, each with different trade-offs. <strong>RRT (Rapidly-exploring Random Tree)</strong> quickly finds feasible paths by randomly sampling the configuration space and growing a tree from start to goal, prioritizing speed over optimality—ideal for real-time applications where any collision-free solution suffices. <strong>RRT</strong>* extends RRT with rewiring to asymptotically approach optimal paths as planning time increases, producing smoother, shorter trajectories at the cost of longer planning times (typically 0.5-2 seconds vs. RRT&#x27;s 0.1-0.5 seconds). <strong>STOMP (Stochastic Trajectory Optimization for Motion Planning)</strong> uses trajectory optimization rather than sampling, generating smooth, natural-looking motions by minimizing a cost function incorporating smoothness, obstacle avoidance, and joint limits. For humanoid arms, STOMP often produces superior execution quality because it considers trajectory dynamics, but requires more computation. MoveIt allows you to configure multiple planners per planning group and select dynamically based on task requirements (quick motions during navigation vs. careful placement near obstacles).</p>
<p><strong>Collision Checking</strong> is performance-critical—planners may evaluate thousands of configurations per query, and each requires collision detection against environment geometry (tables, walls, detected objects) and self-collision (arm hitting torso, hands colliding). MoveIt uses <strong>FCL (Flexible Collision Library)</strong> for geometric collision checking with hierarchical bounding volume trees (enabling fast rejection of distant geometry) and exact mesh-based collision detection for close proximity. For humanoid robots, self-collision checking is particularly important: arms have large workspaces that overlap with torso, head, and the opposite arm. MoveIt&#x27;s Allowed Collision Matrix (ACM) lets you specify which link pairs never collide (e.g., shoulder link and upper arm always connected) to avoid redundant checks. Depth-based collision checking augments geometric methods by treating depth camera point clouds as collision objects, enabling dynamic scene updates: as objects move or humans enter the workspace, the planning scene updates in real-time, and subsequent plans avoid the new obstacles.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="112-motion-planning-for-humanoid-arms">11.2 Motion Planning for Humanoid Arms<a href="#112-motion-planning-for-humanoid-arms" class="hash-link" aria-label="Direct link to 11.2 Motion Planning for Humanoid Arms" title="Direct link to 11.2 Motion Planning for Humanoid Arms" translate="no">​</a></h2>
<p>Configuring MoveIt 2 for humanoid arms requires defining the <strong>kinematic chain</strong> (the sequence of joints and links from base to end-effector), specifying <strong>planning groups</strong> (which joints participate in motion planning), and choosing between <strong>Cartesian space</strong> goals (end-effector pose) and <strong>joint space</strong> goals (explicit joint angles). These configuration decisions fundamentally affect what motions the robot can plan and execute, and understanding the trade-offs enables you to design manipulation systems that balance capability, reliability, and performance.</p>
<p><strong>Kinematic Chain Configuration</strong> starts with your URDF model defining the arm&#x27;s structure: a typical 7-DOF humanoid arm includes shoulder (3 DOF: pitch, roll, yaw), elbow (1 DOF: pitch), and wrist (3 DOF: pitch, roll, yaw) joints. MoveIt requires a SRDF (Semantic Robot Description Format) that augments the URDF with planning-specific metadata: which joints form planning groups, which link serves as the end-effector (typically <code>hand_link</code> or <code>gripper_link</code>), virtual joints connecting the robot base to a world frame, and the Allowed Collision Matrix. For humanoid robots, you typically define multiple planning groups: <code>right_arm</code> (7 joints, end-effector: right hand), <code>left_arm</code> (7 joints, end-effector: left hand), <code>both_arms</code> (14 joints total for bimanual planning), and optionally <code>upper_body</code> including torso joints if your humanoid has a flexible spine. The choice of planning group determines the dimensionality of the configuration space: planning with <code>right_arm</code> searches a 7D space, while <code>both_arms</code> searches 14D, requiring significantly more computation but enabling coordinated bimanual motions.</p>
<p><strong>End-Effector and Inverse Kinematics</strong> define how you specify goals. When you command &quot;move the right hand to position (x, y, z) with orientation (roll, pitch, yaw),&quot; MoveIt must find joint angles that achieve this end-effector pose—this is the <strong>inverse kinematics (IK)</strong> problem. For 7-DOF arms (redundant, since 6 DOF suffice for arbitrary pose), infinite solutions exist, and IK solvers must choose among them based on optimization criteria (minimize joint motion, avoid joint limits, maintain current posture). MoveIt supports multiple IK solvers: <strong>KDL (Kinematics and Dynamics Library)</strong> provides fast, analytical IK for common kinematic structures but can fail to find solutions near singularities; <strong>TracIK</strong> combines analytical and numerical methods for improved reliability and speed; <strong>BioIK</strong> uses evolutionary optimization to handle complex constraints (minimize elbow height, keep gripper upright). For humanoid manipulation, TracIK is generally recommended—it achieves 99%+ success rates on reachable poses while maintaining sub-millisecond solve times, critical for real-time replanning when objects or obstacles move.</p>
<p><strong>Cartesian Path Planning vs. Joint Space Planning</strong> represent fundamentally different motion specifications. <strong>Joint space goals</strong> specify explicit target joint angles: &quot;move to [shoulder: 30°, elbow: 90°, wrist: 0°].&quot; This is computationally efficient (no IK required, direct configuration-to-configuration planning) and guarantees success if a collision-free path exists, but provides no control over end-effector path—the hand might take an unexpected route through space. <strong>Cartesian goals</strong> specify end-effector pose: &quot;move hand to (0.5, 0.3, 1.2)&quot; with orientation from quaternion. MoveIt solves IK at the goal pose, then plans in joint space to reach that configuration. <strong>Cartesian path planning</strong> goes further: &quot;move the hand along a straight line in Cartesian space from current position to target position.&quot; MoveIt samples waypoints along the line, solves IK at each waypoint, and connects them with joint-space interpolation. This produces predictable end-effector trajectories (critical for tasks like &quot;pour water from pitcher&quot; where tilting path matters), but can fail if any waypoint is unreachable or if the straight-line path encounters obstacles.</p>
<p><strong>Planning Time and Quality Trade-offs</strong> require tuning based on application demands. Increasing MoveIt&#x27;s <code>planning_time</code> parameter (default: 5 seconds) allows samplers more iterations to explore the configuration space, producing shorter, smoother paths, but delays execution. For interactive manipulation (&quot;user asks robot to hand them object&quot;), minimize planning time (1-2 seconds) at the cost of suboptimal paths. For batch tasks (&quot;sort 50 objects&quot;), invest in longer planning (5-10 seconds per motion) to reduce execution time and energy consumption. The <code>num_planning_attempts</code> parameter (default: 10) causes MoveIt to run the planner multiple times with different random seeds, selecting the best result—this dramatically improves success rate for difficult queries (cluttered environments, near-singularity configurations) but multiplies planning time. Adaptive planning strategies work well: attempt fast planning first (RRT, 1 second), and if it fails or produces poor-quality paths, fallback to slower, higher-quality planning (RRT*, 5 seconds, or STOMP with trajectory optimization).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="113-grasp-planning-and-execution">11.3 Grasp Planning and Execution<a href="#113-grasp-planning-and-execution" class="hash-link" aria-label="Direct link to 11.3 Grasp Planning and Execution" title="Direct link to 11.3 Grasp Planning and Execution" translate="no">​</a></h2>
<p>Motion planning solves how to move the arm, but <strong>grasp planning</strong> answers the equally critical question: <em>where</em> and <em>how</em> should the gripper contact the object to achieve stable, reliable grasps? Poor grasp planning leads to dropped objects, unstable grasps that fail during manipulation, or collisions between gripper and environment. Effective grasp planning combines geometric reasoning (shape analysis to identify stable contact points), learned models (data-driven prediction of grasp success from visual input), and execution strategies (approach trajectories, gripper force control) to achieve robust manipulation across diverse objects and scenarios.</p>
<p><strong>Grasp Pose Generation</strong> starts by identifying candidate grasp configurations—positions and orientations where the gripper contacts the object. For <strong>parallel-jaw grippers</strong> (most common in humanoid robotics), grasp poses specify where the gripper axis aligns with the object, with jaws opening to a width that accommodates the object&#x27;s local geometry. Geometric approaches analyze object shape: point cloud segmentation identifies graspable regions (cylinders suggest side grasps, flat surfaces suggest top-down pinch grasps), principal component analysis finds dominant axes for alignment, and antipodal grasp generation places gripper jaws on opposing sides with surface normals pointing toward each other (ensuring force closure). For unknown objects, sample-based methods generate many candidate grasps at different positions, orientations, and approach angles, then score and rank them. <strong>Multi-finger grippers</strong> (3+ fingers with multiple DOF per finger) dramatically increase complexity—grasp synthesis must compute contact points for multiple fingertips that collectively immobilize the object, requiring optimization over contact locations and finger joint configurations simultaneously.</p>
<p><strong>Grasp Quality Metrics</strong> score candidate grasps to select the best option. <strong>Geometric metrics</strong> evaluate grasp stability without execution: force closure (can the gripper resist arbitrary wrenches?), minimum wrench (worst-case force/torque the grasp can resist), grasp isotropy (grasp quality uniform across directions). <strong>Heuristic metrics</strong> incorporate task-specific preferences: avoid grasps near object edges (reduce slip risk), prefer grasps with vertical approach (simpler motion planning), maximize distance to obstacles (reduce collision risk), align gripper with task constraints (for &quot;pour water,&quot; grasp handle with pouring spout upward). <strong>Learned metrics</strong> use data-driven models trained on thousands of grasp attempts: <strong>GQCNN (Grasp Quality Convolutional Neural Network)</strong> takes depth images of objects and candidate grasp positions as input, predicting probability of success; <strong>GraspNet</strong> trains on large-scale datasets of diverse objects with ground-truth stable grasps to predict grasp success directly from point clouds. For production humanoid systems, hybrid approaches work best: generate candidate grasps geometrically (fast, explainable), score with learned models (accurate, handles novel objects), and filter with heuristics (incorporate task constraints).</p>
<p><strong>Approach and Retreat Trajectories</strong> frame the grasp within a complete pick sequence: the robot cannot simply teleport the gripper to the grasp pose—it must approach from a safe distance, execute the grasp, and retreat with the object without collisions. The standard pipeline: 1) Compute <strong>pre-grasp pose</strong> by offsetting the grasp pose backward along the gripper approach axis (typically 10-15 cm), ensuring the pre-grasp is collision-free; 2) Plan motion from current configuration to pre-grasp using MoveIt; 3) Execute <strong>Cartesian approach</strong> along a straight line from pre-grasp to grasp pose while opening the gripper; 4) <strong>Close gripper</strong> to specified width or until contact sensors detect object (force/torque thresholds); 5) Execute <strong>Cartesian retreat</strong> along the approach axis (reverse direction) by 10-15 cm to lift object clear of surface; 6) Plan motion to placement location or nominal &quot;holding&quot; configuration. This decomposition isolates the grasp contact phase (steps 3-5) where Cartesian control is critical, allowing motion planning (steps 2, 6) to handle the less constrained reaching motions.</p>
<p><strong>Gripper Control and Force Management</strong> distinguish successful grasps from dropped objects. For <strong>position-controlled grippers</strong> (common servos), command gripper width slightly smaller than object width, relying on mechanical compliance or current limiting to avoid crushing. For <strong>force-controlled grippers</strong> (instrumented with force/torque sensors), close until contact is detected (force threshold exceeded), then apply constant grasping force (typically 5-20 N for household objects, higher for industrial parts). Humanoid grippers often feature tactile sensors (BioTac, capacitive skin) enabling slip detection: if the object begins sliding within the grasp during manipulation, increase gripper force reactively. During trajectory execution, monitor gripper state continuously—if the gripper opens unexpectedly (object slipped), halt motion immediately and signal failure to higher-level task planner for recovery (re-grasp, alert user).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="114-pick-and-place-and-dual-arm-coordination">11.4 Pick-and-Place and Dual-Arm Coordination<a href="#114-pick-and-place-and-dual-arm-coordination" class="hash-link" aria-label="Direct link to 11.4 Pick-and-Place and Dual-Arm Coordination" title="Direct link to 11.4 Pick-and-Place and Dual-Arm Coordination" translate="no">​</a></h2>
<p>Assembling individual manipulation primitives into complete tasks requires <strong>task-level orchestration</strong>—sequencing perception, planning, execution, and error handling into reliable pipelines. Pick-and-place, the canonical manipulation task, integrates everything we&#x27;ve covered: perception identifies object location, grasp planning determines how to grasp it, motion planning computes collision-free paths, and execution monitors success. Extending to <strong>dual-arm coordination</strong> unlocks bimanual capabilities (steadying objects while manipulating them, cooperative lifting of large items, hand-offs between arms) but introduces synchronization challenges and dramatically increases planning complexity.</p>
<p><strong>Pick-and-Place Pipeline</strong> follows a standard state machine: 1) <strong>Detect</strong>: use object detection (Isaac ROS, YOLO) or segmentation to identify target object and estimate 6D pose; 2) <strong>Plan Grasp</strong>: generate and score grasp candidates, select best grasp pose relative to detected object pose, transform to robot base frame; 3) <strong>Plan Approach</strong>: compute motion plan from current configuration to pre-grasp pose, checking for collisions with environment and detected obstacles; 4) <strong>Execute Approach</strong>: send trajectory to controller, open gripper; 5) <strong>Execute Grasp</strong>: Cartesian approach to grasp pose, close gripper, verify grasp success (force sensors, gripper position feedback); 6) <strong>Plan Transport</strong>: with object now attached to gripper (update MoveIt planning scene with attached collision object), plan motion to placement location pre-place pose; 7) <strong>Execute Transport</strong>: send trajectory, continuously monitor for grasp failure; 8) <strong>Execute Place</strong>: Cartesian approach to placement pose, open gripper, retreat. Each step includes <strong>failure detection</strong>: if object detection fails (object not visible), if grasp planning fails (no stable grasps found), if motion planning fails (no collision-free path), if grasp execution fails (object dropped), trigger recovery behaviors (adjust viewpoint, try alternative grasp, request human assistance).</p>
<p><strong>Dual-Arm Planning</strong> presents a choice between <strong>independent</strong> and <strong>coordinated</strong> modes. Independent planning treats each arm separately: plan right arm motion ignoring left arm (except avoiding collisions with it), plan left arm motion ignoring right arm. This is computationally efficient (two 7D planning problems) and works well when arms operate in separate workspaces. However, collision checking must be conservative—each arm&#x27;s motion must avoid not just the other arm&#x27;s current configuration but its entire swept volume during simultaneous motion, potentially leading to overly cautious plans. <strong>Coordinated planning</strong> treats both arms as a single 14-DOF system, planning in the joint configuration space of both arms simultaneously. This enables true bimanual motions (both arms cooperatively manipulating a single large object), optimized solutions where one arm yields workspace to let the other pass, and precise relative positioning (holding object steady with left hand while right hand manipulates). The cost is computational: 14D planning takes orders of magnitude longer than 7D, often requiring simplified planners (straight-line interpolation, pre-computed motion primitives) or task-specific constraints that reduce effective dimensionality.</p>
<p><strong>Object Handoffs</strong> exemplify coordinated manipulation complexity: one arm (donor) grasps an object, both arms move to handoff pose with grippers in proximity, receiving arm grasps object, donor arm releases and retreats. Synchronization is critical—the receiving arm must grasp before the donor releases, requiring precise timing (ROS 2 action coordination) and spatial alignment (handoff pose must be reachable by both arms with compatible grasp orientations). Force control improves robustness: both grippers apply moderate force during handoff overlap, ensuring the object is secure before donor releases. For humanoid robots assisting humans, mixed-initiative handoffs extend this to human-robot interaction: the robot detects the human reaching to receive an object (vision-based hand detection), extends arm to comfortable handoff location, monitors grasp force to detect when human has grasped object, then releases. Research has shown that embodied learning approaches can significantly improve object-centric robotic manipulation, allowing robots to adapt grasp and handoff strategies based on object properties and learned experiences (Springer, 2025).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercises">Exercises<a href="#exercises" class="hash-link" aria-label="Direct link to Exercises" title="Direct link to Exercises" translate="no">​</a></h2>
<ol>
<li class=""><strong>MoveIt Setup</strong>: Configure MoveIt 2 for a simulated humanoid arm</li>
<li class=""><strong>Motion Planning</strong>: Plan and execute motion to multiple target poses</li>
<li class=""><strong>Grasp Planning</strong>: Generate and execute grasp poses for various objects</li>
<li class=""><strong>Pick-and-Place</strong>: Complete full pick-and-place task in Isaac Sim</li>
<li class=""><strong>Dual-Arm Task</strong>: Coordinate both arms to hand off an object</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<ul>
<li class="">MoveIt 2 provides production-ready manipulation planning for ROS 2</li>
<li class="">Motion planning requires trade-offs between speed and optimality</li>
<li class="">Grasp planning combines geometric and learned approaches</li>
<li class="">Dual-arm coordination enables complex bimanual tasks</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class="">MoveIt 2 official documentation: <a href="https://moveit.ros.org/" target="_blank" rel="noopener noreferrer" class="">https://moveit.ros.org/</a></li>
<li class="">OMPL (Open Motion Planning Library): <a href="https://ompl.kavrakilab.org/" target="_blank" rel="noopener noreferrer" class="">https://ompl.kavrakilab.org/</a></li>
<li class="">MoveIt 2 tutorials and setup guides for ROS 2</li>
<li class="">Grasp planning algorithms: GraspIt!, GQCNN, DexNet, GraspNet</li>
<li class="">Motion planning algorithms comparison (RRT, RRT*, PRM, STOMP)</li>
<li class="">Bimanual manipulation and dual-arm coordination research</li>
<li class="">TracIK: Trac-IK inverse kinematics solver documentation</li>
<li class="">FCL (Flexible Collision Library): <a href="https://github.com/flexible-collision-library/fcl" target="_blank" rel="noopener noreferrer" class="">https://github.com/flexible-collision-library/fcl</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<p>Machine Intelligence Research, Springer. (2025). A Survey of Embodied Learning for Object-centric Robotic Manipulation. <em>Machine Intelligence Research</em>. <a href="https://link.springer.com/article/10.1007/s11633-025-1542-8" target="_blank" rel="noopener noreferrer" class="">https://link.springer.com/article/10.1007/s11633-025-1542-8</a></p>
<hr>
<p><strong>Status</strong>: ✅ Content complete (2,000 words) - Phase 11 drafted 2025-12-13</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book/tree/main/book/docs/11-manipulation.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Book/docs/10-voice-to-action"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">10. Voice to Action</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Book/docs/12-sim-to-real"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">12. Sim-to-Real</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#111-moveit-2-architecture" class="table-of-contents__link toc-highlight">11.1 MoveIt 2 Architecture</a></li><li><a href="#112-motion-planning-for-humanoid-arms" class="table-of-contents__link toc-highlight">11.2 Motion Planning for Humanoid Arms</a></li><li><a href="#113-grasp-planning-and-execution" class="table-of-contents__link toc-highlight">11.3 Grasp Planning and Execution</a></li><li><a href="#114-pick-and-place-and-dual-arm-coordination" class="table-of-contents__link toc-highlight">11.4 Pick-and-Place and Dual-Arm Coordination</a></li><li><a href="#exercises" class="table-of-contents__link toc-highlight">Exercises</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/01-intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Book/docs/13-capstone">Capstone Project</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.omniverse.nvidia.com/isaacsim/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/humaafatima/physical-ai-humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI Robotics Team. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>